{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Investment Opportunity Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD AN OUTLINE HERE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start with importing necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2 #to read images\n",
    "import glob #to tell it what kind of files to read within the filepath, in this case .jpg's\n",
    "import skvideo.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image   # for preprocessing the images\n",
    "from tensorflow.keras.utils import to_categorical #np_utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Activation, MaxPooling2D, Conv2D, InputLayer)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import double, jit, njit, vectorize\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, ConfusionMatrixDisplay)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle\n",
    "\n",
    "from skimage.transform import resize   #for resizing images\n",
    "\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tool we are making is designed work from images ALONE, the wrangling phase is to get the images into the right format.\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the path to read all of the images \n",
    "\n",
    "#Will use 80/20 split for training/testing, following Ahmed/Moustafa model study\n",
    "\n",
    "#Training\n",
    "path_training_flip = glob.glob('Houses Dataset/Training Set/Flip_Mixed_Training/Flip_Kitchen_Training/*.jpg')\n",
    "path_training_noflip = glob.glob('Houses Dataset/Training Set/Dont Flip_Mixed_Training/Dont Flip_Kitchen_Training/*.jpg')\n",
    "\n",
    "#lol wow so that last slash at the very end actually DOES matter!! it didn't work without it - was getting 'empty set' error\n",
    "#but it won't tell you that here for some reason\n",
    "\n",
    "#Testing\n",
    "path_testing_flip = glob.glob('Houses Dataset/Testing Set/Flip_Mixed_Testing/Flip_Kitchen_Testing/*.jpg')\n",
    "path_testing_noflip = glob.glob('Houses Dataset/Testing Set/Dont Flip_Mixed_Testing/Dont Flip_Kitchen_Testing/*.jpg')\n",
    "\n",
    "#glob looks for all files/filepaths that follow/contain a specified pattern, using the *wild card, so here *.jpg!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sA innocent beginnings!:\n",
    "#so just thought of something - the point for testing is IT'S supposed to tell us whether it thinks it should be a flip/\n",
    "#not flip... so i was like then why are WE giving it the answer right off the bat!? but the thing is it doesn't realize\n",
    "#we're telling it the answer. we're only doing this for our OWN purposes for organization to make it easy for ourselves/\n",
    "#to see/compare ITS answers to the RIGHT answers!\n",
    "#wait actually so... how does it work to tell you the 'accuracy' of the prediction at the end?\n",
    "#like cuz you label the flip/not flip. so like is it just taking those images and tryna slap a label on and then\n",
    "#comparing to your label? how does it not cheat/get bias? how does it do this?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that'll get each image ready for modeling / preprocess\n",
    "\n",
    "def image_preprocessing(path):\n",
    "    #so with this function we'll give it the filepath. in fact, amazingly, this is the ONLY argument required!!\n",
    "    \n",
    "    #empty list to store all finished, preprocessed images\n",
    "    images = []\n",
    "\n",
    "    #loop through all the path images, preprocessing each one\n",
    "    #remember the path uses GLOB so it's only getting .jpg's in our case since that's what we defined/wildcarded. there should only be .jpg's in there anyway, but just in case!\n",
    "    for i in path:\n",
    "        #read the image\n",
    "        img = cv2.imread(i)\n",
    "        # Adjust the size so all iamges will be the same size\n",
    "        img = cv2.resize(img, dsize = (32,32), interpolation=cv2.INTER_CUBIC) #changed from raghu's 70, 140\n",
    "        #interpolation tells it how to \"create new pixels\" to make the image look smoother as you make it bigger, aka *FILL-IN* the gaps!!!\n",
    "        #rather than just stretch the original pixels and risking \"pixely\" images\n",
    "        #bicubic is the smoothest cuz it's curved/polynomial\n",
    "\n",
    "        # Crop to remove excess of the images we don't need for modeling, like around the border/near the edges, unnecessary noise,\n",
    "        #watermarks, text etc\n",
    "        \n",
    "        #okay so we just set the pic above to be 32 pixel width x 32 height\n",
    "        #so now we're cropping it to be MIDDLE 28 x 28! basically, it starts @ pixel 2, in each dimension, then goes 28, to pixel 30!\n",
    "\n",
    "        #y,x are the STARTING pixel location height, width address/coords\n",
    "        #to START trim at / KEEP picture at, then h,w is THE ACTUAL DESIRED LENGTH OF THE PICTURE!!!\n",
    "        #img = img[y:y+h, x:x+w] (#so if you're okay with STARTING AT 0, then ofc would just be img[y:h, x:w]!)\n",
    "        #so we ofc keep the option open bc prob in MOST cases we wouldn't be starting at 0!/the edge!\n",
    "        #so like this is setting the pixels we wanna KEEP! so let's say you've got 100x100 pixels, and you wanted to trim centered/evenly,\n",
    "        #so that you take 20 pixels off the top and 20 pixels off each side - top,bottom,left, right; aka the *MIDDLE 60*!!!\n",
    "        #so like you'd want pixels 20 thru *80*! thus, it'd be: img = img[20:60+20,...] aka 20:80!!!\n",
    "        #so since/if (0,0) is top left, then this example of 0,100,0,70 will keep only TOP 100 pixels of 140 and\n",
    "        #chop off BOTTOM 40!! but he's okay w/ keeping ALL the width\n",
    "        \n",
    "        #SO THE y & x ARE THE *STARTING* y,x pixel point you wanna begin crop at/start keeping at!!!\n",
    "        #then of course the h & w are actual height and width in pixels, so to get the right crop you gotta treat like\n",
    "        #this,- algebraically/addressly/plotly, so that, say you want the new cropped pic to be only the MIDDLE 100\n",
    "        #vertical pixels in heighth, out of 140 original, then you gotta START AT 20, then \"ADD\" 100 to represent the\n",
    "        #desired image size, which'll take it to 120,-> THE DESIRED END LENGTH PIXEL LOCATION!!!\n",
    "        \n",
    "        ####QUESTION???####\n",
    "        \n",
    "        \n",
    "        #the way they do it is starting pixel & number of pixels to go from there, aka the DESIRED PIXEL LENGTH!\n",
    "        y,h,x,w = 2,28,2,28\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        \n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5 #contrast/gain\n",
    "        beta=0.5  #brightness/bias\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        #addWeighted helps to blend/transition two images together, by specifiying respective weights, like how visible\n",
    "        #or transparent one is\n",
    "        #the np.zeros thing is the 2nd image?? it's creating an image of 0 size and 0 dtype? what's 0 dtype?\n",
    "        #is this just a way of creating a 'blank' image to make it\n",
    "        #then what is 0?? the template is:\n",
    "        #cv.addWeighted(image1, alpha, image2, beta, gamma[, dst[, dtype]]) #note this is cv and not cv2... any difference?\n",
    "        #this is confusing cuz the *0* is in the BETA spot and BETA is in the GAMMA spot! mistake??\n",
    "        #also more confusing cuz alpha & beta here are the respective WEIGHTS to give to each image\n",
    "        #whereas above it had to do w/ contrast & brightness?\n",
    "        #and also, it seems like for image2 we're creating a 'blank' image, which we're further giving 0 weight...\n",
    "        #which poses the question - why even bother doing a blend if we're not blending?? we're only using the first\n",
    "        #image by itself! well maybe we're blending an image with a blank image to attempt to create a picture with a\n",
    "        #'transparent' background, like those png images! to remove any excess / nonessential background noise around the\n",
    "        #main part / meat of the picture?\n",
    "        #but also, shouldn't alpha & beta be on a scale of 0 to 1 and sum to 1?? Or actually, I guess you could look at\n",
    "        #it as each one can be an independent percentage transparency of its full resolution, so they can both be 1\n",
    "        #for example and so don't necess have to add up to 1... BUT they DO both still need to be some RATIO / can't\n",
    "        #be more than 1! but alpha is 1.5??\n",
    "        \n",
    "        \n",
    "        \n",
    "        ####QUESTIONS???!!!####\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Normalize the images to be black and white by reverting the images and then dividing by 255.0\n",
    "        \n",
    "        #this would be an important step in applications where color is irrelevant and it's just noise and you wanna focus\n",
    "        #on the features. but in our application, REAL ESTATE, color is definitely very important. like if the floors are\n",
    "        #modern gray, that property will def have more value\n",
    "        \n",
    "        img = cv2.bitwise_not(img)  #can look up this function later\n",
    "        img = img/255               #and can look up why divide by 255 >> TO REDUCE ALL THE PIXEL COLOR VALUES\n",
    "        #(described by a set of 3 numbers) TO BETWEEN 0 & 1!!! CUZ THE COLOR RATING/VALUE RANGE IS FROM 0 TO 255!!!\n",
    "        #ORIGINALLY I THOUGHT THIS WAS DIVIDING BY 255 TO GET EVERYTHING IN GRAYSCALE LOL SO I COMMENTED OUT!!\n",
    "\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "        # Create the video\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images\n",
    "\n",
    "#okay so overall, this is iterating thru our images in our path folder and reshaping/resizing/recoloring them and\n",
    "#tryna crop out the background noise as much as possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the training data\n",
    "\n",
    "#so in his example/template, he's classifying diff scenarios - pages to flip and pages that shouldn't be flipped\n",
    "#what am I doing?  well, i guess in a way, I'M deciding whether to FLIP or not too.... A PROPERTY!!!\n",
    "#so that means I needa manually sort thru each set of pics and train it based on which houses I want renovated and\n",
    "#which ones I want as is\n",
    "#so gotta fine/re-tune the objective/purpose/goal here. cuz we could make it like that^, or we could do more closely\n",
    "#to what we were initially thinking, which is more like the Big Mountain project, and is what the creators of the\n",
    "#parent dataset project i'm using did - just take ALL the data, split randomly, and use those to train factors to come\n",
    "#up with the predicted price? oh but then we'd need text data - we'd have to have baseline prices\n",
    "#i'm tryna remember how we did it in Big Mountain - we had prices, and then i think: found the most important factors\n",
    "#that influenced the prices, and then accordingly used those to PREDICT what prices SHOULD be, based off what everyone\n",
    "#else was doing. Similarly, this Ahmed/Moustafa project was for HOUSE PRICE ESTIMATION based off images AND text/numerical\n",
    "#data both!\n",
    "\n",
    "#but I was told it gets tricky when you try to do both, even though that makes more sense - that's obviously how a human\n",
    "#would work, but to do that w/ computers takes advanced neural networking / deep learning etc and I'm not quite at that\n",
    "#point yet. So, we'll simplify and use images alone! that's why this makes sense that we would have to do CATEGORIES then-\n",
    "#because this is traditionally used as/or at least one very common/famous example/use of this is for CLASSIFICATION!\n",
    "#so basically, image/face recognition like Apple's FaceID & furry friends & laying out all the 'people' in/from your pictures\n",
    "#so you can quickly go to the pictures w/ them! and of course the first famous one that i knew - Facebook's facial recognition\n",
    "#for tagging suggestions where they look at your pictures, look at the faces in them, and cross ref w/ your friends'\n",
    "#pictures to get matches (lol what if your friend only has like a dog / only ever has dogs as their profile pic - no humans\n",
    "#so that anytime you have a dog in your pic it suggests that it's that person! >v<)\n",
    "#so yeah, then if we're only doing images, we don't have any prices to train it on, we can only pre-break it up and feed\n",
    "#it what's what - so what properties should be:\n",
    "#Renovated/Flipped - ones that are in poorer condition\n",
    "\n",
    "\n",
    "#would be great to learn how to do both images and text so we can look at/consider other factors like neighborhood etc!\n",
    "#and look at the asking price, come up with a predicted ACTUAL selling price, and then estimate calculations for the cost\n",
    "#of renovating and the potential PROFIT MARK-UP / PRICE WE CAN SELL IT AT ONCE WE RENOVATE!! and also give the price\n",
    "#we can get if we decide to RENT IT OUT!!! and then of course provide an accompanying report of like a cash flow/RoR\n",
    "#analysis!!! It may be a slightly different algorithm for rental properties as far as like what costs go into it cuz\n",
    "#may not spend on the same things cuz renovating for different purposes/diff audience. completely diff cash recovery/return\n",
    "#method. so the cash flow obviously will look completely diff, diff costs etc\n",
    "#so in that ideal scenario, the categories could be like: Renovate-Flip To Sell, Renovate-Flip To Rent,\n",
    "#Buy-As-Is=>>TURNKEY To Rent, or Pass\n",
    "#(could even have it look for rental properties you wanna renovate and SELL and not rent out yourself!)\n",
    "#factor in whether to pay cash or finance\n",
    "\n",
    "#but for now, we'll keep it / start off simple and just classify as Renovate/Flip or Pass/Not Flip\n",
    "\n",
    "\n",
    "#HMMMmmmm, but now that i look more closely at this dataset I have, these are almost all NICE houses that are ready to\n",
    "#go and wouldn't be candidates for flipping. and these are all sfh's i believe so may not have alot of options for renting\n",
    "#either, unless there are some small ones. but again, identifying / subclassifying for renting is outside of the\n",
    "#scope of this most likely since we don't have text data to tell us what's a multi fam vs. a condo etc and don't have\n",
    "#the square footage and 'num of dwellings' to support that\n",
    "\n",
    "#################################################################################################################################\n",
    "#SO - that may mean that i need to MANUALLY collect/develop/compile my OWN database of images!!!! both for training and testing purposes\n",
    "#################################################################################################################################\n",
    "\n",
    "\n",
    "####QUESTION!!!####\n",
    "#In image machine learning, do we also train it on what does NOT constitute a category, so that it doesn't get confused\n",
    "#by other things/special/rare circumstances it might occur and knows how to handle it?\n",
    "#YES!! that's what the NOT_FLIP is for!!\n",
    "#for example, with real estate, in general we'll teach it to look for stuff that's outdated, based on style and color\n",
    "#(and oftentimes even low image quality alone will indicate a bad situation i.e. a GOOD opportunity to renovate/flip\n",
    "#but i guess it wouldn't matter too much cuz would still needa base on elements of image). also ARRANGEMENT - like if\n",
    "#things are messy/in disarray. but what about if it encounters stuff that, technically, yes, is outdated, but it's in\n",
    "#SEVERE disrepair, abandoned! how will / do we teach it to NOT classify those as investment opportunities / flips but\n",
    "#rather as Do Not Buy's!\n",
    "#so we may need to stick to just one kind of property, i.e. single family homes, and not do like condos or townhomes\n",
    "#if we're factoring in the EXTERIOR of buildings cuz that would proabably throw it off and would be better to keep those\n",
    "#all separate at first and then combine / aggregate later\n",
    "#BUT - we COULD use ONLY interior pictures, in which case it wouldn't matter if it's a SFH, townhome or condo - and\n",
    "#actually that makes more sense to only use interior bc that's the MAIN BASIS for deciding whether something is a good\n",
    "#flip or not because that's the MAIN DRIVER of price in people's minds. cuz think about it - in a condo highrise or\n",
    "#townhome village, - the exterior is shared/identical! so what sets them apart that can drastically alter price!?\n",
    "#(other than possibly view) >> interior design!!!\n",
    "\n",
    "\n",
    "#img_training = image_preprocessing(path = path_training_flip)\n",
    "#Don't need to write path since that's the only arg / in general don't need if following/aligned w/ order\n",
    "#deleted the remainder originals - all like this\n",
    "\n",
    "# Read the training not flip\n",
    "\n",
    "img_training_flip = image_preprocessing(path_training_flip)\n",
    "\n",
    "# Read the training not flip\n",
    "\n",
    "img_training_noflip = image_preprocessing(path_training_noflip)\n",
    "\n",
    "# Read the test flip\n",
    "\n",
    "img_testing_flip = image_preprocessing(path_testing_flip)\n",
    "\n",
    "# Read the test not flip\n",
    "\n",
    "img_testing_noflip = image_preprocessing(path_testing_noflip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the labels\n",
    "\n",
    "#so this will output 1 for EVERY element in the folder?? the label is just gonna be 1 for each element?\n",
    "#and what is the y representing anyway?\n",
    "#ohhh, wait, think i got it. so we wanna train this thing to know when to flip and when not to flip, so \"DO flip\"\n",
    "#is 1, aka TRUE!!! and don't is 0, seen below\n",
    "#so we're assigning a value of 1/True for EVERY item in the training\n",
    "#i have to look back at Big Mountain but i think we used labels when we were labeling the points as/w/ the state names\n",
    "#but here the label also represents the value\n",
    "\n",
    "#im a little confused on doing it for the testing set tho? again gotta look at Big Mountain but why are we assigning its\n",
    "#values? isn't the point that we're supposed to SEE how well it does, like it SHOULD result in 1's for flips and 0's\n",
    "#for nonflips... so maybe it's just gonna use these as benchmarks of what they SHOULD/'VE BEEN so we can see how the\n",
    "#actuals compare to these shoulds/ideals/theoreticals\n",
    "y_train_flip = [1 for i in range(0, len(img_training_flip))]\n",
    "\n",
    "y_train_noflip = [0 for i in range(0, len(img_training_noflip))]\n",
    "\n",
    "y_test_flip = [1 for i in range(0, len(img_testing_flip))]\n",
    "\n",
    "y_test_noflip = [0 for i in range(0, len(img_testing_noflip))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X_train, X_test, y_train and y_test for analysis\n",
    "\n",
    "#hmm okay, so the X's are the images themselves, interesting! that's all there is to it - can summarize all of it\n",
    "#to simply that!\n",
    "#and the y's are what we saw before/above - 1's & 0's accordingly\n",
    "#and as we see, combine/unsegregate the flips/notflips into one! cuz together they still make up the training/testing sets\n",
    "\n",
    "#Note the difference b/w concat & append! concat defaults to stacking VERTICALLY -> axis = 0\n",
    "#and append defaults to FLATTENING COMPLETELY -> fully unraveling / stretching / connecting / daisy-chaining\n",
    "#ALL rows out to ONE SINGLE LONG ROW!!!\n",
    "\n",
    "#can do any kind of 3 stackings w/ either, but best to go w/ one who has what you're looking for as the default\n",
    "#so that's why we use concat for X, cuz we want those vertically stacked,\n",
    "#whereas we want our y's as one single long row\n",
    "#... wait WHY THO????\n",
    "#so for the X's, the flips & non-flips images will be two separate rows\n",
    "#for the y's, the flips & non-flips VALUES will be ONE SINGLE ROW!\n",
    "#question is - which direction are img_ & train_flip orientated?? which way does append naturally stack?\n",
    "#okay, so i did a basic append using an empty list, which is what it seems they did w/ image_processing, and it\n",
    "#APPENDS SIDEWAYS!!! horizontal/across/left-to-right. so then our hunch was right - concatting 2 X lists of images - those\n",
    "#lists are each sideways (as opposed to upright; they're 'supine' lol), so when we concat w/ axis=0, they'll be stacked\n",
    "#vertically, rowswise, so yes, each of these new X vars will be 2 rows, one flip one nonflip\n",
    "#okay and then checked for y's by doing a practice list comprehension and that ALSO created a sideways/-wise list!\n",
    "#thus, appending two y lists on axis None will make one longe list/row!!! and actually, didn't even need to practice\n",
    "#check cuz anytime doing axis None for either (concat or append), it'll first AUTOMATICALLY FLATTEN the composite\n",
    "#lists regardless of orientation! so that way it can just daisy-chain them together!\n",
    "#so not sure if there's a reason *WHY* both are oriented horizontal, but maybe that's the key point - maybe it's not\n",
    "#that they have to be oriented one way or the other, as long as they're oriented the SAME direction!\n",
    "\n",
    "\n",
    "#####QUESTION#####\n",
    "\n",
    "#X's are two rows\n",
    "X_train = np.concatenate((img_training_flip, img_training_noflip))#, axis = 0) #default axis is already 0!!!\n",
    "\n",
    "X_test = np.concatenate((img_testing_flip, img_testing_noflip))#, axis = 0) #can leave for emphasis/making it clear\n",
    "\n",
    "#y's are one long row\n",
    "y_train = np.append(y_train_flip, y_train_noflip)#, axis = None)\n",
    "\n",
    "y_test = np.append(y_test_flip, y_test_noflip)#, axis = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 28, 28, 3)\n",
      "(133,)\n",
      "(47, 28, 28, 3)\n",
      "(47,)\n"
     ]
    }
   ],
   "source": [
    "# See if the shapes match between the X_train and y_train and the X_test and y_test\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#okay so 462 for training bc we got 253 items for flip & 209 for don't flip\n",
    "#BUT WHAT THE HECK IS THE 100, 70, 3????\n",
    "#isn't it only even possible for the shape to be 2-dimensional???\n",
    "#or at least, isn't that what it is here, since it's two rows??\n",
    "#and/but actually it's a little confusing cuz the two rows have diff num of columns\n",
    "#cuz first row is flip, which has more images than non-flip - so don't know if that causes issues?\n",
    "#not sure how Raghu's was divided between flip/not flip. but his also shows the 100, 70, 3\n",
    "#and it shows that for both training and testing???\n",
    "# and we got 123 items for testing bc we got 72 items for flip & 51 for don't flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.7254902 , 0.56862745, 0.34901961],\n",
       "         [0.61568627, 0.45490196, 0.25490196],\n",
       "         [0.74509804, 0.54509804, 0.29019608],\n",
       "         ...,\n",
       "         [0.76862745, 0.63137255, 0.4745098 ],\n",
       "         [0.80392157, 0.6745098 , 0.4627451 ],\n",
       "         [0.40392157, 0.25098039, 0.03921569]],\n",
       "\n",
       "        [[0.59607843, 0.43137255, 0.23137255],\n",
       "         [0.82745098, 0.67843137, 0.4745098 ],\n",
       "         [0.66666667, 0.53333333, 0.34509804],\n",
       "         ...,\n",
       "         [0.61960784, 0.41568627, 0.18039216],\n",
       "         [0.78431373, 0.63137255, 0.35686275],\n",
       "         [0.34117647, 0.20784314, 0.        ]],\n",
       "\n",
       "        [[0.44313725, 0.2745098 , 0.09019608],\n",
       "         [0.6745098 , 0.49019608, 0.24313725],\n",
       "         [0.81568627, 0.70196078, 0.45098039],\n",
       "         ...,\n",
       "         [0.50196078, 0.33333333, 0.10196078],\n",
       "         [0.71372549, 0.58039216, 0.28627451],\n",
       "         [0.64705882, 0.4745098 , 0.20784314]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.8627451 , 0.71372549, 0.49803922],\n",
       "         [0.83137255, 0.70980392, 0.48627451],\n",
       "         [0.80392157, 0.71372549, 0.56078431],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.00784314],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.80392157, 0.6745098 , 0.4627451 ],\n",
       "         [0.78039216, 0.65098039, 0.43921569],\n",
       "         [0.85098039, 0.78431373, 0.62745098],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.00784314],\n",
       "         [0.        , 0.        , 0.01176471],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.80392157, 0.66666667, 0.45490196],\n",
       "         [0.78431373, 0.64313725, 0.43137255],\n",
       "         [0.65490196, 0.49019608, 0.38431373],\n",
       "         ...,\n",
       "         [0.        , 0.00784314, 0.03921569],\n",
       "         [0.        , 0.        , 0.02745098],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.66666667, 0.26666667, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.56862745, 0.61960784, 0.60392157],\n",
       "         [0.63921569, 0.65490196, 0.61568627],\n",
       "         [0.4745098 , 0.46666667, 0.43529412]],\n",
       "\n",
       "        [[0.6627451 , 0.25098039, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.38431373, 0.38431373, 0.35686275],\n",
       "         [0.38039216, 0.36862745, 0.34117647],\n",
       "         [0.44313725, 0.42745098, 0.39607843]],\n",
       "\n",
       "        [[0.66666667, 0.25098039, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.31372549, 0.30980392, 0.2627451 ],\n",
       "         [0.39215686, 0.37254902, 0.32156863],\n",
       "         [0.76470588, 0.67843137, 0.58039216]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.81960784, 0.43921569, 0.07843137],\n",
       "         [0.85490196, 0.53333333, 0.14901961],\n",
       "         [0.0627451 , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.11372549, 0.16862745, 0.10196078],\n",
       "         [0.21960784, 0.22745098, 0.15294118],\n",
       "         [0.45098039, 0.45098039, 0.38823529]],\n",
       "\n",
       "        [[0.83137255, 0.4627451 , 0.10980392],\n",
       "         [0.85490196, 0.53333333, 0.15686275],\n",
       "         [0.07843137, 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.65098039, 0.68627451, 0.6627451 ],\n",
       "         [0.78039216, 0.82745098, 0.80392157],\n",
       "         [0.18431373, 0.25098039, 0.21568627]],\n",
       "\n",
       "        [[0.85490196, 0.50196078, 0.1372549 ],\n",
       "         [0.81568627, 0.50196078, 0.1372549 ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.90980392, 0.94901961, 0.97254902],\n",
       "         [0.82745098, 0.8627451 , 0.85882353],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.12156863, 0.01960784, 0.        ],\n",
       "         [0.11372549, 0.00392157, 0.        ],\n",
       "         [0.1372549 , 0.01960784, 0.        ],\n",
       "         ...,\n",
       "         [0.58039216, 0.49019608, 0.41960784],\n",
       "         [0.41568627, 0.34509804, 0.2627451 ],\n",
       "         [0.34117647, 0.22352941, 0.12941176]],\n",
       "\n",
       "        [[0.13333333, 0.04313725, 0.        ],\n",
       "         [0.1254902 , 0.03137255, 0.        ],\n",
       "         [0.10980392, 0.00392157, 0.        ],\n",
       "         ...,\n",
       "         [0.43137255, 0.2627451 , 0.11372549],\n",
       "         [0.27843137, 0.19215686, 0.10588235],\n",
       "         [0.2745098 , 0.11372549, 0.        ]],\n",
       "\n",
       "        [[0.09019608, 0.04313725, 0.        ],\n",
       "         [0.12156863, 0.05098039, 0.        ],\n",
       "         [0.09803922, 0.00784314, 0.        ],\n",
       "         ...,\n",
       "         [0.40392157, 0.20784314, 0.09019608],\n",
       "         [0.30980392, 0.19607843, 0.11372549],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6627451 , 0.65098039, 0.64313725],\n",
       "         [0.65490196, 0.63137255, 0.62745098],\n",
       "         [0.67843137, 0.65098039, 0.64313725],\n",
       "         ...,\n",
       "         [0.21960784, 0.1254902 , 0.05098039],\n",
       "         [0.35686275, 0.22745098, 0.12156863],\n",
       "         [0.34509804, 0.25490196, 0.15686275]],\n",
       "\n",
       "        [[0.6627451 , 0.64313725, 0.63921569],\n",
       "         [0.67843137, 0.65490196, 0.65098039],\n",
       "         [0.65098039, 0.62745098, 0.61568627],\n",
       "         ...,\n",
       "         [0.12156863, 0.05098039, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.34117647, 0.24705882, 0.19215686]],\n",
       "\n",
       "        [[0.65098039, 0.62745098, 0.61960784],\n",
       "         [0.78039216, 0.76078431, 0.75686275],\n",
       "         [0.58039216, 0.54901961, 0.5372549 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.03921569, 0.        , 0.        ],\n",
       "         [0.02745098, 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.81568627, 0.64313725, 0.09019608],\n",
       "         [0.91372549, 0.7372549 , 0.22745098],\n",
       "         [0.86666667, 0.67843137, 0.29803922],\n",
       "         ...,\n",
       "         [0.54509804, 0.24313725, 0.        ],\n",
       "         [0.6745098 , 0.3372549 , 0.        ],\n",
       "         [0.73333333, 0.38039216, 0.        ]],\n",
       "\n",
       "        [[0.84313725, 0.69019608, 0.14901961],\n",
       "         [0.97254902, 0.82745098, 0.32156863],\n",
       "         [0.88627451, 0.71372549, 0.32156863],\n",
       "         ...,\n",
       "         [0.55686275, 0.23137255, 0.        ],\n",
       "         [0.68627451, 0.35686275, 0.        ],\n",
       "         [0.70980392, 0.31764706, 0.        ]],\n",
       "\n",
       "        [[0.85490196, 0.6745098 , 0.09019608],\n",
       "         [0.98431373, 0.85490196, 0.38431373],\n",
       "         [0.81568627, 0.60392157, 0.19607843],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.65098039, 0.34509804, 0.        ],\n",
       "         [0.79215686, 0.52941176, 0.0745098 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.71372549, 0.46666667, 0.14901961],\n",
       "         [0.70980392, 0.39607843, 0.1254902 ],\n",
       "         [0.62745098, 0.35686275, 0.07843137],\n",
       "         ...,\n",
       "         [0.25098039, 0.10980392, 0.        ],\n",
       "         [0.45098039, 0.25490196, 0.16862745],\n",
       "         [0.85098039, 0.65098039, 0.41960784]],\n",
       "\n",
       "        [[0.65490196, 0.36078431, 0.05490196],\n",
       "         [0.70980392, 0.41568627, 0.08627451],\n",
       "         [0.63137255, 0.3254902 , 0.05098039],\n",
       "         ...,\n",
       "         [0.42745098, 0.32156863, 0.0745098 ],\n",
       "         [0.29019608, 0.10196078, 0.        ],\n",
       "         [0.7254902 , 0.55294118, 0.34509804]],\n",
       "\n",
       "        [[0.67843137, 0.40392157, 0.1254902 ],\n",
       "         [0.74901961, 0.4627451 , 0.20392157],\n",
       "         [0.70980392, 0.4745098 , 0.14901961],\n",
       "         ...,\n",
       "         [0.49803922, 0.36862745, 0.19607843],\n",
       "         [0.3254902 , 0.18431373, 0.        ],\n",
       "         [0.78039216, 0.55686275, 0.27058824]]],\n",
       "\n",
       "\n",
       "       [[[0.27843137, 0.        , 0.        ],\n",
       "         [0.29019608, 0.        , 0.        ],\n",
       "         [0.29803922, 0.01960784, 0.        ],\n",
       "         ...,\n",
       "         [0.39215686, 0.41960784, 0.2745098 ],\n",
       "         [0.63921569, 0.71372549, 0.74901961],\n",
       "         [0.50980392, 0.60392157, 0.56078431]],\n",
       "\n",
       "        [[0.8745098 , 0.60784314, 0.39215686],\n",
       "         [0.2745098 , 0.        , 0.        ],\n",
       "         [0.27843137, 0.00392157, 0.        ],\n",
       "         ...,\n",
       "         [0.39215686, 0.45490196, 0.38039216],\n",
       "         [0.54509804, 0.66666667, 0.62352941],\n",
       "         [0.34117647, 0.34901961, 0.22745098]],\n",
       "\n",
       "        [[0.38431373, 0.20784314, 0.        ],\n",
       "         [0.52156863, 0.31372549, 0.03137255],\n",
       "         [0.64313725, 0.45490196, 0.24313725],\n",
       "         ...,\n",
       "         [0.39215686, 0.49803922, 0.4627451 ],\n",
       "         [0.40784314, 0.45490196, 0.45098039],\n",
       "         [0.28627451, 0.27058824, 0.10588235]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.21568627, 0.        , 0.        ],\n",
       "         [0.49803922, 0.20784314, 0.        ],\n",
       "         [0.2627451 , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.25098039, 0.        , 0.        ],\n",
       "         [0.61960784, 0.4745098 , 0.23137255],\n",
       "         [0.54509804, 0.34117647, 0.06666667]],\n",
       "\n",
       "        [[0.3372549 , 0.04313725, 0.        ],\n",
       "         [0.24313725, 0.        , 0.        ],\n",
       "         [0.27843137, 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.19607843, 0.        , 0.        ],\n",
       "         [0.49803922, 0.24313725, 0.        ],\n",
       "         [0.62745098, 0.48627451, 0.21568627]],\n",
       "\n",
       "        [[0.28627451, 0.        , 0.        ],\n",
       "         [0.40392157, 0.10980392, 0.        ],\n",
       "         [0.31372549, 0.01960784, 0.        ],\n",
       "         ...,\n",
       "         [0.16862745, 0.        , 0.        ],\n",
       "         [0.25098039, 0.        , 0.        ],\n",
       "         [0.61568627, 0.45098039, 0.17647059]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.05490196, 0.00392157, 0.        ],\n",
       "         [0.22352941, 0.13333333, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.09803922, 0.03921569, 0.        ],\n",
       "         [0.21568627, 0.13333333, 0.        ],\n",
       "         [0.20784314, 0.12941176, 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.18039216, 0.09019608, 0.        ],\n",
       "         [0.16078431, 0.06666667, 0.        ],\n",
       "         [0.15686275, 0.05882353, 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.51372549, 0.31372549, 0.        ],\n",
       "         [0.6627451 , 0.45490196, 0.0745098 ],\n",
       "         [0.60392157, 0.40392157, 0.04313725]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.52156863, 0.3254902 , 0.        ],\n",
       "         [0.50980392, 0.32156863, 0.        ],\n",
       "         [0.59215686, 0.37254902, 0.01960784]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.02745098, 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.5254902 , 0.34509804, 0.        ],\n",
       "         [0.53333333, 0.3372549 , 0.        ],\n",
       "         [0.55294118, 0.34901961, 0.        ]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oh yeah bruh! why don't we just see what these arrays even look like now that we have them/can!!\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm interesting... so that's why the shape is so complicated lol? the shape i guess isn't simply the rows & columns\n",
    "#w/ an array\n",
    "#like np.shape([[1, 3]]) >> (1, 2) bc there's one [list] with 2 elements\n",
    "#np.shape(np.array([(1, 2), (3, 4), (5, 6)]) >> (3,) bc it's 3 comma'd elements, but why no 1?....\n",
    "\n",
    "#okay so the numpy array shape is: (#dimensions, #elements in that dimension)\n",
    "#so each set of brackets is a dimension\n",
    "#so X(_train) has 462 images, so each one is a bracketed set, then each of those has a 70x3 list-of-lists, i.e. 70\n",
    "#bracketed sets, 3 elements each. this must have something to do w/ the image processing where this is how it\n",
    "#transcribes images as numerical data structures - must be based on the image size/number of pixels we specified which\n",
    "#is why it'd be the same in his data set and mine!!\n",
    "#prob gotta adjust the image size!!! based on what's appropriate given my image sizes\n",
    "#it could be that each set of 3 is THE COLOR PROFILE FOR ONE PIXEL!!! must be cuz you can see all the 255's which\n",
    "#represent black i believe!!! (we divided by 255 earlier to convert each pixel's color component value to scale b/w 0 & 1!)\n",
    "#AHHH OKAY YES!!! SO WE SPECIFIED IN THE IMAGE_PREPROCESSING TO MAKE THE HEIGHT AND WIDTH OF EACH PICTURE 100 X 70\n",
    "#(pixels i guess?) HEIGHT AND WIDTH!!!!!\n",
    "#okay so now it all makes sense - there's 462 images, and each of those is 100 x 70 pixels, and each pixel is composed\n",
    "#of/described by 3 color values. so there's 100 rows \n",
    "#you can see that there's 4 sets of brackets which may be why there's 4 elements in the shape?\n",
    "#OKAY ALHAMDULILLAH ALLAH GAVE ME THE ANSWER!!!:\n",
    "#each pixel is described by 3 color values, so there's 70 sets [brackets] of 3 across (width pixels),\n",
    "#and then 100 rows of those 70x3 sets of pixels going down (height pixels) -> which makes up the complete image for\n",
    "#ONE single images\n",
    "#and there's 462 images so then there's 462 sets of these 100 x 70 x 3 sets of pixels :D!!!!!\n",
    "#so i guess it's kinda irrelevant that there's 2 rows lol!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's do same for y\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay yeah so what we expected, and much easier to read/understand visually. it's simply ONE LONG single list\n",
    "#although it's a bit confusing cuz i thought the shape reps the dimensions and a dimension is signified by ONE / EACH\n",
    "#set of brackets and this is just one single?\n",
    "#interesting, so if there's just one single set of brackets then it'll be (462,) #second element 0?\n",
    "#so you CAN have just one set of brackets, AS LONG AS THERE'S NO INNER/SUB BRACKETS!!! like can't do np.array([1,2], [3,4])!\n",
    "#always gotta have an OUTER/WRAPPER pair/set of brackets\n",
    "#but for this case of just one set of brackets, the second is optional. if you DO do the second, then the shape'll be\n",
    "#(462,1)!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new array that will have the original arrays (labels and values) but they will be shuffled. \n",
    "\n",
    "# Create the array for the train data set\n",
    "# well this is just a(n empty) LIST, but we make it an array later at the end. but can we just\n",
    "#make it an/initialize an empty array NOW??\n",
    "\n",
    "X_train_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_train):\n",
    "    # The new array would be the array containing the image plus its label\n",
    "    \n",
    "    \n",
    "    #Hmm so enumerate prints the element along w/ its index number as a tuple pair\n",
    "    #but X_train is just the set horiz list of combined flip & non-flip iamges\n",
    "    #but this is saying it'll give the image + its 'label'. what's its label?\n",
    "    #we talked about / set labels for the y's above, as 1's or 0's\n",
    "    #but how do we tie those to these images?\n",
    "    \n",
    "    #so enumerate(X_train) will give like [(0, 'img0'), (1, 'img1'), etc...]\n",
    "    #so the i,j in this refer to the index position and the image, respectively-> (index, image)\n",
    "    #so, this 'new_array' below makes (image, label)\n",
    "    #ahh okay. so y_train is comprised of an array of 1's for/from y_train_flip & an array of 0's from\n",
    "    #y_train_notflip. it's a single, combined, 1-row flattened array of all one's followed by all 0's\n",
    "    #it's number of elements was designed to be exactly matched to that of/ the number of images in its respective folder\n",
    "    #so y_train combined is the total number of training images, both flip and not flip\n",
    "    #now X_train is the combined set of images, but the first row is flips and second is NOT flips!\n",
    "    #similarly it's all the flips followed by all the nonflips\n",
    "    #SO the images and their corresponding values/labels are all slated to line up!! now just need to pair them together,\n",
    "    #which is exactly the point of enumerating/tupling!\n",
    "    #so new_array = (j, y_train[i]) pairs them off by taking j image sequentially from X_train and pairs it off w/\n",
    "    #the sequential value from/in y_train, which is lined up so it corresponds to it! so like [('img0', 1), ('img1', 1), etc...]\n",
    "    #did we need to do that though?\n",
    "    #couldn't we have just directly done:\n",
    "    #for i,j X_train, y_train: \n",
    "        #new_array = (X_train[i], y_train[j])???\n",
    "    #or even just concatted the 2 tables together?\n",
    "    #like wouldn't have been really easy to simply start w/ the X_ arrays and simply add a column to them of all 1's\n",
    "    #and all 0's as needed?\n",
    "    #ohhh, it could be because we want them as inseparable/interlocked PAIRS cuz we're gonna SHUFFLE THEM SO NEED THEM\n",
    "    #TO STAY INTACT!!!! can't do that w/ simple rows/columns/df format. this is more secure. cuz tuples are IMMUTABLE!!!\n",
    "    #but couldn't we spit out tuples directly w/ /create tuple pairs using ZIP, like simply:\n",
    "    #new = zip(X_train, y_train) --> so - this WOULD work if we were dealing with LISTS but NOT arrays! if you try this\n",
    "    #w/ arrays, it'll simply make ONE SINGLE TUPLE out of the WHOLE first array and WHOLE second array!\n",
    "    #i tried doing using a for loop but didn't work either:\n",
    "    #for x, y in keys, values:\n",
    "        #zip(keys[x], values[y])\n",
    "        #error: 'not enough values to unpack (expected 2, got 1)'...\n",
    "        #tried as/w inner loop/loop-w/in-loop, but that errored too\n",
    "        #also tried as lists instead of arrays, but that said 'TOO many values to unpack'! lol - but this'd be an\n",
    "        #unnecessarily long way to do it anyway for lists. if you got lists you got it easy!\n",
    "    #however, these 2 things probably COULD have been set up as lists instead of arrays\n",
    "    #BUT - i'm guessing there was probably a reason to set up as arrays, like maybe many of the other functions could\n",
    "    #only be done if np.arrays, like shuffling\n",
    "    #also, instead of appending new_array to X_train_shuffle,...\n",
    "    #WHY DIDN'T WE JUST DO IT DIRECTLY!!!\n",
    "    #like instead of new_array made it X_train_shuffle\n",
    "    #would we even had to have initalized it? cuz didn't initialize new_array!!! thus wouldn't need to/no need to\n",
    "    #do .append either if we did that!!!\n",
    "    #oh wait, note. so initially the i refers to the INDEX num of X_train, cuz that's the part of enumerate\n",
    "    #but then we're calling y_train[i]... well i guess that's fine. kinda like a neat workaround. iteration runs the same\n",
    "    #number/order as indexes, so that works\n",
    "    #hmmm, even when i tried to replicate this and do a for-loop w/ enumerate w/ arrays, it still acted the same as simply doing zip!!!\n",
    "    #so i wonder-- is that what we're actually going for? or, does it just LOOK like this output/display but in actuality\n",
    "    #it is tupled the way we want? i.e. in effect/practice, the elements are PAIRED-WEDDED-BONDED the way we want???\n",
    "    \n",
    "    \n",
    "    \n",
    "    #####QUESTION#####\n",
    "\n",
    "\n",
    "    \n",
    "    new_array = (j, y_train[i])\n",
    "    # Append the values to the array (empty LIST we initialized) that will be shuffled\n",
    "    X_train_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays\n",
    "X_train_shuffle = np.array(X_train_shuffle)\n",
    "\n",
    "\n",
    "#okay so we CALL IT \"shuffle\" but it's not shuffled at all? it's perfectly lined up? does that come later or something?\n",
    "#why not do it now and finish it off?\n",
    "#coulda done here:\n",
    "#np.random.shuffle(X_train_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array for the test data set\n",
    "\n",
    "X_test_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_test):\n",
    "    # The new array would be the array containing the image plus its label \n",
    "    new_array = (j, y_test[i])\n",
    "    # Append the values to the array that will be shuffled\n",
    "    X_test_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays  \n",
    "X_test_shuffle = np.array(X_test_shuffle)\n",
    "\n",
    "#coulda done here:\n",
    "#np.random.shuffle(X_test_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the random shuffle to make the train and test with no specific order\n",
    "\n",
    "np.random.shuffle(X_train_shuffle)\n",
    "\n",
    "np.random.shuffle(X_test_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate between the X_train and y_train to fit the model\n",
    "\n",
    "#so now that we shuffled the PAIRS after zipping them together, we can now UNZIP / separate them back to in to independent lists\n",
    "#which is what we'll need to train the model, which takes each as separate arguments an X independent and y dependent,\n",
    "#but THEIR ORDER / LINING-UP-NESS WILL STILL BE INTACT!\n",
    "\n",
    "#we already have these arrays tho w/ these names... so we're overwriting them??\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Start a for loop into the X_train_shuffle\n",
    "for i in X_train_shuffle:  #remem, X_train_shuffle is a set of shuffled up PAIRS of images w/ their corresponding values\n",
    "                           #so that's the great thing - we don't risk mixing up the data since we're shuffling PAIRS!!! keeping them intact\n",
    "    #so each 'i' is a tuple pair of (image, value)\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0] #ohh okay, so this is referencing the 0-index of EACH i-element-tuple-pair within X_train_shuffle,\n",
    "    #which is the IMAGE!! cuz i([[0],[1]]) = X_train_shuffle([[0],[1]]) = X_train_shuffle([[image],[value]])!!!\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1] #and so the 1-index-position of each tuple-pair-element in X_train_shuffle is the value!!!\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_train.append(value)\n",
    "    y_train.append(label)\n",
    "    \n",
    "#so it's just peeling apart / separating out the pair into it's parts! and putting into the respective bucket/bins/indep lists!\n",
    "#: 1st element / value / X / image into the X list and 2nd element / label / y / classifier to the y list!\n",
    "\n",
    "#hmmm okay, so now we store ALL the X_train images in one list(? or array?), SHUFFLED\n",
    "#oh - check below - again, not shure why didn't make arrays directly up here but yeah, these above as they stand are LISTS!\n",
    "#and do the same for the y-values\n",
    "#BUT REMEMBER! they were shuffled TOGETHER! as PAIRS! so these will STILL line up if we wanna join em back together!\n",
    "#and if they're lists we can even re-tuple/combine them now easily!\n",
    "\n",
    "\n",
    "#but again - question is - why do we wanna separate em into diff lists? i get it tho that if this is something we wanted\n",
    "#to do, we couldn't have simply taken these lists independently CUZ THE WHOLE POINT WAS TO TUPLE-PAIR-TIE THEM!!!\n",
    "    \n",
    "#but is it necessary to do a for loop to get the 'columns' of this tupled array? hmm doesn't seem so - remem how this\n",
    "#'tupled' array object looks like? seems to behave how you'd expect off that - nothing you can really do w/ it?\n",
    "\n",
    "# Divide between X_train and y_train to run model\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the test data set\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Start a for loop into the X_test_shuffle\n",
    "for i in X_test_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_test.append(value)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 28, 28, 3)\n",
      "(133,)\n",
      "(47, 28, 28, 3)\n",
      "(47,)\n"
     ]
    }
   ],
   "source": [
    "# Make sure shapes are the same as their originals\n",
    "#bc all we did, after all that, was essentially SHUFFLE it - didn't alter / edit / add / remove any data/points!\n",
    "\n",
    "#?\n",
    "#this shows checking the shapes/lengths of each of these arrays after we JUST updated them\n",
    "#so only way to check the shape against the originals is if we made new names for these, OR have those printed out above\n",
    "#also, this says/(said) 'Make sure labels are the same as the first shapes' --> i think instead of 'labels' it means\n",
    "#shape presumably? so i changed it/wording\n",
    "\n",
    "#give new names above so we can differentiate and keep rather than overwrite and compare side by side here! put text\n",
    "#like print('The shape of X_train originally was %X_train.shape% and now it is %X_train_shuffled.shape%')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function with the neural networks\n",
    "\n",
    "def neural_network():  #what does it mean if it has no args again?\n",
    "    model = Sequential() #?\n",
    "    #whoaa - what does all this mean??\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', #removed kernel_initializer='he_uniform'\n",
    "                     padding = 'same', input_shape=(28, 28, 3))) #so this is the second part of the shape it gives for\n",
    "    #oh had to change the input_shape from 100, 70, 3 to match my changes: 28, 28, 3\n",
    "    model.add(MaxPooling2D((2, 2))) #our lists, as seen right above, that i don't understand\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu')) #removed kernel_initializer='he_uniform'\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#######WOWZA!!!! CAN'T WAIT TO LEARN THIS!!!##########\n",
    "\n",
    "#AHMED/MOUSTAFA:!!!\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.layers.convolutional import Conv2D\n",
    "# from keras.layers.convolutional import MaxPooling2D\n",
    "# from keras.layers.core import Activation\n",
    "# from keras.layers.core import Dropout\n",
    "# from keras.layers.core import Dense\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Input\n",
    "# from keras.models import Model\n",
    "\n",
    "# def create_mlp(dim, regress=False):\n",
    "# \t# define our MLP network\n",
    "# \tmodel = Sequential()\n",
    "# \tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "# \tmodel.add(Dense(4, activation=\"relu\"))\n",
    "\n",
    "# \t# check to see if the regression node should be added\n",
    "# \tif regress:\n",
    "# \t\tmodel.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "# \t# return our model\n",
    "# \treturn model\n",
    "\n",
    "# def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
    "# \t# initialize the input shape and channel dimension, assuming\n",
    "# \t# TensorFlow/channels-last ordering\n",
    "# \tinputShape = (height, width, depth)\n",
    "# \tchanDim = -1\n",
    "\n",
    "# \t# define the model input\n",
    "# \tinputs = Input(shape=inputShape)\n",
    "\n",
    "# \t# loop over the number of filters\n",
    "# \tfor (i, f) in enumerate(filters):\n",
    "# \t\t# if this is the first CONV layer then set the input\n",
    "# \t\t# appropriately\n",
    "# \t\tif i == 0:\n",
    "# \t\t\tx = inputs\n",
    "\n",
    "# \t\t# CONV => RELU => BN => POOL\n",
    "# \t\tx = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "# \t\tx = Activation(\"relu\")(x)\n",
    "# \t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "# \t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "# \t# flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "# \tx = Flatten()(x)\n",
    "# \tx = Dense(16)(x)\n",
    "# \tx = Activation(\"relu\")(x)\n",
    "# \tx = BatchNormalization(axis=chanDim)(x)\n",
    "# \tx = Dropout(0.5)(x)\n",
    "\n",
    "# \t# apply another FC layer, this one to match the number of nodes\n",
    "# \t# coming out of the MLP\n",
    "# \tx = Dense(4)(x)\n",
    "# \tx = Activation(\"relu\")(x)\n",
    "\n",
    "# \t# check to see if the regression node should be added\n",
    "# \tif regress:\n",
    "# \t\tx = Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "# \t# construct the CNN\n",
    "# \tmodel = Model(inputs, x)\n",
    "\n",
    "# \t# return the CNN\n",
    "# \treturn model\n",
    "\n",
    "\n",
    "# # create our Convolutional Neural Network and then compile the model\n",
    "# # using mean absolute percentage error as our loss, implying that we\n",
    "# # seek to minimize the absolute percentage difference between our\n",
    "# # price *predictions* and the *actual prices*\n",
    "# model = create_cnn(64, 64, 3, regress=True)\n",
    "# opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "# model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "\n",
    "# # train the model\n",
    "# print(\"[INFO] training model...\")\n",
    "# model.fit(trainImagesX, trainY, validation_data=(testImagesX, testY),\n",
    "# \tepochs=200, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x28fd54a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x28fd54a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6926 - accuracy: 0.4762\n",
      "Epoch 2/15\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 0.6850 - accuracy: 0.5625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 11:51:09.835150: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-11 11:51:09.835309: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.5319\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6864 - accuracy: 0.5121\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6716 - accuracy: 0.5780\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6638 - accuracy: 0.5409\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6670 - accuracy: 0.5213\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6652 - accuracy: 0.5078\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6531 - accuracy: 0.5657\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6436 - accuracy: 0.7166\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6358 - accuracy: 0.7830\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6292 - accuracy: 0.7595\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6226 - accuracy: 0.7399\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6174 - accuracy: 0.7436\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6104 - accuracy: 0.7902\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6093 - accuracy: 0.7690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28fd9a100>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "#little confusing - so we're naming our call of the neural_network function as 'model'\n",
    "#but in the function model = Sequential()\n",
    "#and then everything is built on top of / supplemented to 'model'\n",
    "#like what would happen if we named the below something else? is it like w/ the way we wrote the function they/it has\n",
    "#to match?\n",
    "\n",
    "model = neural_network()\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning displayed from above. similar one below too w/ predictions:\n",
    "# Epoch 1/15\n",
    "# WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x29c4860d0> and will run it as-is.\n",
    "# Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
    "# Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
    "# To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
    "# WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x29c4860d0> and will run it as-is.\n",
    "# Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
    "# Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
    "# To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
    "#  1/15 [=>............................] - ETA: 2s - loss: 0.7007 - accuracy: 0.6250\n",
    "# 2022-01-07 12:22:18.922563: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
    "# 2022-01-07 12:22:18.922707: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x28f0c0b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x28f0c0b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions (predict the 'y'-value, aka the label, whether it's a flip or not flip, based on the training\n",
    "#we just did above)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get them into 0 and 1 values\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "# Start a for loop to iterate over the predictions array\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:\n",
    "        binary_values.append(1)\n",
    "\n",
    "#but how does it know what a 0 means and what a 1 means? like we assigned the 0's and 1's earlier for/as the\n",
    "#correct answers. but how does it know to call a flip a 1 and a don't flip a 0?\n",
    "#ohhh okay i get it! so we TRAINED it to identify things as either a 0 or a 1!!! cuz it can only understand numbers!\n",
    "#it has no idea what they actually are or what they mean/represent!!! thus it's PREDICTIONS will be like a weighted\n",
    "#average of how much it leans toward it being a 0/notflip or 1/flip. and we force it to pick one cuz ultimately it\n",
    "#needs to make a decision. in the final analysis of accuracy since there's diff metrics i'm sure there's on that looks\n",
    "#at the actual value to see how close it was. kinda like pass/fail but there of course still has to be an actual score!\n",
    "#it's just that you have to draw a line/make a cutoff somewhere\n",
    "        \n",
    "#hmm okay, so it's training/fitting the model using X_train & y_train, and then from that we make predictions\n",
    "#of the values of X_test to see how they compare to y_test! and so remem the predicted values will be WEIGHTED in their\n",
    "#raw form, so like if we're classifying w/ 2 things/choices, it can only be one or the other, but it will spit back\n",
    "#like *HOW MUCH IT THINKS/leans toward it being one or the other*, and so we have to tell it at what point/percentage/\n",
    "#confidence we want it to make a call for one or the other, what's the defining/differentiating/determining/\n",
    "#identifying/distinguishing/DECISION line?!!!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning displayed above:\n",
    "# WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x29e361ee0> and will run it as-is.\n",
    "# Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
    "# Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
    "# To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
    "# WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x29e361ee0> and will run it as-is.\n",
    "# Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
    "# Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
    "# To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the confusion matrix to evaluate the model\n",
    "\n",
    "#what's that!??\n",
    "#ohhhh okay, i get it now. look at the axes. so you're seeing how well the common labels lined up. i.e. where an image\n",
    "#was both PREDICTED to be a flip AND was ACTUALLY a flip, as determined/stated by the label we gave it!\n",
    "#so in time of writing this, it's showing, of the 102 test images, all 72 were categorized as flips were indeed labeled\n",
    "#as flips. but actually,... EVERYTHING was labeled as a flip!!! so all 50 NOT FLIPS were ALSO labeled as flips!! it\n",
    "#didn't label ANYTHING as a not flip!!! lol\n",
    "\n",
    "cm = confusion_matrix(y_test, binary_values)\n",
    "#so the y_test is now giving the ANSWER KEY so the algorithm can see how it did / its performance on this test/section\n",
    "#so we gave the algo/machine this test set of images, combined flip and not flip so it didn't know which was which,\n",
    "#all it had was the model it was trained on w/ the training set of images and corresponding labels. that was the\n",
    "#instruction/guided section. this was its turn to do it on its own/try its hand at it\n",
    "#so you see that it's actually very simple here, it's comparing the machine's answers, in 'binary_values', defined/\n",
    "#assigned above, and comparing them to the CORRECT ANSWERS, as defined by us in y_test!!! confusion matrix is simply\n",
    "#just a visual tool to help us see the performance w/ machine learning prediction performance\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['dont_flip','flip'])\n",
    "cmd.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after fixing the /255 scaling, of the 123 test images (72 flips & 51 dont flips) test images, it got almost all the\n",
    "#dont flips right, but then also labeled the same number of flips as don't flip...\n",
    "\n",
    "#after changing the image size to 32 x 32 and just cropping slightly to 28 x 28, jumped up to 63% accuracy\n",
    "#it got a good chunk of the flips right, 58/72, but more than half of the DON'T flips wrong\n",
    "\n",
    "#okay then i decided to focus on JUST kitchens to see if that would make thing better, but again lopsided results\n",
    "#took care to trim out dataset to take out potentially confusing ones, although still had mix of semi-modern and\n",
    "#modern modern. but it did really well w/ the non flips, but then also said not to flip most of the flips?\n",
    "#so maybe i confused it w/ too many diff kinds of examples\n",
    "#lol, this shows you how good of a teacher you are and also gives you insight into how a child's brain works...\n",
    "\n",
    "\n",
    "#nice!! okay so after narrowing down types to make more consistent the model is performing much better and isn't so\n",
    "#lopsided - now it's doing well over 50% for both flip & dont flip\n",
    "\n",
    "#so next will probably be to continue to narrow/trim down to TYPES of kitchens, to get it superfocustrained on one/each\n",
    "#type and also will prob need to get more images!\n",
    "\n",
    "#hmm but then when i narrowed down further, it did even better for flips, but basically labeled EVERYTHING as a flip,\n",
    "#so labeled most of the nonflips as flips\n",
    "\n",
    "#so there could be several factors\n",
    "#there's the varying number of flip training vs. not flip training - see what they did. well actually,\n",
    "#ahmed/moustafa's experiment wasn't classification! so no categories to compare\n",
    "#there were a few images of diff starting size - maybe threw it off\n",
    "#there was that alpha/beta manipulation\n",
    "#there's stuff ahmed/moustafa did w/ the images that maybe we didn't do here, or diff versions perhaps of same type of\n",
    "#methods, like see their CNN function compared to vs our NN\n",
    "#>>>>they do addl things like create an MLP, seems like they pretty much import the same packages from keras/tensorflow?\n",
    "#and also diff VALUES used for same things, like how we saw w/ image size, but also like epochs - they did many more\n",
    "#we might've overcomplicated the model by giving it pics of different rooms? maybe needa break it up?\n",
    "#also there were only a few / one-off (not enough) images of nonflips of exterior without much training on that\n",
    "#remem the training had alot of SEMI-modern images/examples for non-flips, and actually, some were flips,\n",
    "#so might've confused it. so could be like we suspected, may have to pick one or other\n",
    "#could be that there's other things that need to be checked, like where i altered stuff, such as taking out the axis=,etc, or like how i commented out the divide by 255!\n",
    "#check out the messages w/ the model training/epoch part?\n",
    "#there might be some differences w/ ahmed/moustafa since they were also using text. (how) did they tie both image & text\n",
    "#training & testing together? and did they have to compare each room to each room when making a prediction, like apples\n",
    "#to apples? or was that automatically figured out?\n",
    "#>>>>OHH okay i see now as i look thru ahmed/moustafa's code that they PUT EVERY SET OF 4 IMAGES TOGETHER AS A SET!\n",
    "#they make a 2x2 tile block and ensure they're in the same order / arrangement every time. so yeah may well be that we\n",
    "#have to keep it as sets, maybe of 3 cuz as we said, don't care about exterior so much; can do that separately\n",
    "#OR, can pick just ONE thing to start off w/ at first, like a kitchen, to determine if it's outdated or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what's the best way to see which images it identified as which, esp the wrong ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72        24\n",
      "           1       0.71      0.65      0.68        23\n",
      "\n",
      "    accuracy                           0.70        47\n",
      "   macro avg       0.70      0.70      0.70        47\n",
      "weighted avg       0.70      0.70      0.70        47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the Classification report to get the precision, recall, f1-score\n",
    "\n",
    "#whatre thoooooooose!?!? >> it's to simply see the performance/accuracy, based on different standards\n",
    "\n",
    "print(classification_report(y_test, binary_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to create a model with 0.__ accuracy for whether a page needs to be flipped or not by using deep learning and doing the necessary data preprocessing such as making all the images black & white, all the same size, cropping them, adjusting brightness, adding noise, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x296b1cb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x296b1cb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: flip_page_classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 11:51:10.623271: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "# Save the model using pickle\n",
    "\n",
    "#doesn't say pickle anywhere??? we did import it at start/top tho\n",
    "\n",
    "model_classifier = model.save('flip_page_classifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c476d5be062af8c53c5a2e4f951b2f07aa9c47f05899e43528a06f0dd12534bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
