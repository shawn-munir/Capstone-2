{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Profit Opportunity Identification For Improvement\n",
    "\n",
    "## Step_1: Data Wrangling\n",
    "\n",
    "Normally, at least at this level of training, data wrangling involves cleaning up text. But we are attempting something more challenging and sophisticated - wrangling *images*. In fact, this project will involve *only* images and no text/numerical data\n",
    "\n",
    "To do so, we start with this template with packages and tools such as TensorFlow (paired with keras), cv2 & glob\n",
    "\n",
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2 #to read images\n",
    "import glob #to tell it what kind of files to read within the filepath, in this case .jpg's\n",
    "import skvideo.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to divide up these imports because taking forever so need to see what the holdup is\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image   # for preprocessing the images\n",
    "from tensorflow.keras.utils import to_categorical #np_utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Activation, MaxPooling2D, Conv2D, InputLayer)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "#oh nice!!! finally got all of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import double, jit, njit, vectorize\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, ConfusionMatrixDisplay)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle\n",
    "\n",
    "from skimage.transform import resize   # for resizing images\n",
    "\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the path to read all of the images \n",
    "\n",
    "#flip/not flip was the images training on which ones are examples of \"pages to flip or not to flip\" - that was the nature\n",
    "#of this machine learning application\n",
    "\n",
    "# path_training_flip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/training/flip/*.jpg')\n",
    "# path_training_notflip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/training/notflip/*.jpg')\n",
    "# path_testing_flip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/testing/flip/*.jpg')\n",
    "# path_testing_notflip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/testing/notflip/*.jpg')\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "#Will use 80/20 split for training/testing, following Ahmed/Moustafa model study\n",
    "\n",
    "#Training\n",
    "training_path = glob.glob('/Users/deens/OneDrive/Documents/Career/DataScienceMachineLearning/Tools/git/springboard/springboard-data-science/Capstone-2/Houses Dataset/Training Set/*.jpg')\n",
    "\n",
    "#Testing\n",
    "testing_path = glob.glob('/Users/deens/OneDrive/Documents/Career/DataScienceMachineLearning/Tools/git/springboard/springboard-data-science/Capstone-2/Houses Dataset/Testing Set*.jpg')\n",
    "\n",
    "\n",
    "#so what glob does is look for all files/filepaths that follow/contain a specified pattern, using the *wild card,\n",
    "#so here *.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will do all the preprocessing for each image to be ready for modeling\n",
    "\n",
    "def image_preprocessing(path):   #so with this function we'll define the filepath. in fact, that's the ONLY argument\n",
    "                                 #image_preprocessing is just the name we've given to this function\n",
    "    # Create an empty list to store all the preprocessed images\n",
    "    images = []\n",
    "    # Start by creating a for loop through all the path images and make the preprocessing to each image\n",
    "    #so it'll go thru each image / file in the filepath/folder one by one, but remember the path uses GLOB so it's only\n",
    "    #getting .jpg's in our case since that's what we defined/wildcarded/told it to do. there should only be .jpg's in\n",
    "    #there anyway\n",
    "    for i in path:\n",
    "        # Firstly read the image (using the cv2)\n",
    "        img = cv2.imread(i)\n",
    "        # Adjust the size so all iamges will have the same size\n",
    "        img = cv2.resize(img, dsize = (70,140), interpolation=cv2.INTER_CUBIC)\n",
    "        #interpolation tells it how to \"create new pixels\" to make the image look smoother as you make it bigger,\n",
    "        #rather than just stretch the original pixels and risking \"pixely\" images\n",
    "        #bicubic is the smoothest cuz it's curved/polynomial\n",
    "        # Crop to remove excess of the images we don't need for modeling, like around the border/near the edges. unnecessary noise\n",
    "        #watermarks/text etc\n",
    "        y,h,x,w = 0,100,0,70\n",
    "        img = img[y:y+h, x:x+w] #if y & x are 0, why not just do img[y:h, x:w]?\n",
    "        \n",
    "        \n",
    "        ####QUESTION???####\n",
    "        \n",
    "        \n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5 #contrast/gain\n",
    "        beta=0.5  #brightness/bias\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        #addWeighted helps to blend/transition two images together, by specifiying respective weights, like how visible\n",
    "        #or transparent one is\n",
    "        #the np.zeros thing is the 2nd image?? it's creating an image of 0 size and 0 dtype? what's 0 dtype?\n",
    "        #is this just a way of creating a 'blank' image to make it\n",
    "        #then what is 0?? the template is:\n",
    "        #cv.addWeighted(image1, alpha, image2, beta, gamma[, dst[, dtype]]) #note this is cv and not cv2... any difference?\n",
    "        #this is confusing cuz the *0* is in the BETA spot and BETA is in the GAMMA spot! mistake??\n",
    "        #also more confusing cuz alpha & beta here are the respective WEIGHTS to give to each image\n",
    "        #whereas above it had to do w/ contrast & brightness?\n",
    "        #and also, it seems like for image2 we're creating a 'blank' image, which we're further giving 0 weight...\n",
    "        #which poses the question - why even bother doing a blend if we're not blending?? we're only using the first\n",
    "        #image by itself! well maybe we're blending an image with a blank image to attempt to create a picture with a\n",
    "        #'transparent' background, like those png images! to remove any excess / nonessential background noise around the\n",
    "        #main part / meat of the picture?\n",
    "        #but also, shouldn't alpha & beta be on a scale of 0 to 1 and sum to 1?? Or actually, I guess you could look at\n",
    "        #it as each one can be an independent percentage transparency of its full resolution, so they can both be 1\n",
    "        #for example and so don't necess have to add up to 1... BUT they DO both still need to be some RATIO / can't\n",
    "        #be more than 1! but alpha is 1.5??\n",
    "        \n",
    "        \n",
    "        \n",
    "        ####QUESTIONS???!!!####\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Normalize the images to be black and white by reverting the images and then dividing by 255.0\n",
    "        \n",
    "        #this would be an important step in applications where color is irrelevant and it's just noise and you wanna focus\n",
    "        #on the features. but in our application, REAL ESTATE, color is definitely very important. like if the floors are\n",
    "        #modern gray, that property will def have more value\n",
    "        \n",
    "        #img = cv2.bitwise_not(img)  #can look up this function later\n",
    "        #img = img/255               #and can look up why divide by 255\n",
    "\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "        # Create the video\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images\n",
    "\n",
    "#okay so overall, this is iterating thru our images in our path folder and reshaping/resizing/recoloring them and\n",
    "#tryna crop out the background noise as much as possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to create a video from images\n",
    "\n",
    "#uhhh wait, why are we creating a video?? do we need that for our purposes? it's making like a slideshow?\n",
    "\n",
    "def video_creator(path, pathIn, time, fps): #so the user is specifying: (1) the path to get the images from,\n",
    "    frame_video = [] #(2) the path to write the resultant video to, (3) the length of the video?? (don't see this used tho!!),\n",
    "    for i in path: #& (4) the frames per second\n",
    "        \n",
    "        \n",
    "         ####QUESTION???####\n",
    "        \n",
    "        \n",
    "        img = cv2.imread(i)\n",
    "        height, width, layers = img.shape   #what's layers? and i guess all 3 are defined in img.shape -- but where is\n",
    "        size = (width, height)              #that defined?? in above use we seem to have made it 0/0s, but we never\n",
    "        frame_video.append(img)             #stored that as like a variable?? but i guess this is just saying that\n",
    "    out = cv2.VideoWriter(pathIn, cv2.VideoWriter_fourcc(*'mp4v'), fps, size) #make these 3 things equal to WHATEVER\n",
    "    #out is just the output filepath & specs themselves but NOT\n",
    "    #the act of WRITING the output itself?? that's what the below iterator\n",
    "    #is for??\n",
    "    \n",
    "    \n",
    "        ####QUESTIONS???####\n",
    "\n",
    "    \n",
    "    for i in range(len(path)):                                                #img.shape evaluates to - like it's just\n",
    "        out.write(frame_video[i])                                             #a function that can be called\n",
    "    \n",
    "        #why do we do for i in range(len(path)) instead of just for i in path, like we did above??\n",
    "        #it's a way of iterative printing by calling the index rather than simply calling the elements one by one in order\n",
    "        #as it standard/ly does. but isn't calling by index what it's inherently/implicitly/automatically doing??\n",
    "        #what would the length of path be anyway? cuz if path is just a text string for/giving the filepath/directory,\n",
    "        #then isn't len(path) just literally the character count? i.e. number of characters in the path name?\n",
    "        #but maybe this is doing the ACTUAL writing of each image to the out/put path, defined above, in accordance with\n",
    "        #the specs defined in that out/put function, so it's iterating thru each image in the path and making it into\n",
    "        #the final product output video?\n",
    "        #so i think this process is IV/FOUR/4 steps:\n",
    "        #(1) we cycle thru each image, resize,etc it\n",
    "        #(2) append each of theses images to a collective/aggregated 'reel', which is just a collection of images,\n",
    "        #just like how a pdf is a collection of pages, but it's not a MOVIE yet cuz it hasn't been formatted as one/to move!\n",
    "        #that's what step 4 is for! the conversion/formatting. but before that, before the actual piecemeal conversion,\n",
    "        #we gotta:\n",
    "        #(3) set up the specs for the final output video, like as far as format type, fps, size, etc\n",
    "        #(4) write each one of those pics/frames in the collective/tion to the final output conversion/formatting function \"out\"\n",
    "        #for formatting/conversion in accordance w/ the defined/outlined specs to become a / the final video\n",
    "        #product output!\n",
    "        #again tho, couldn't we just have / simply achieved this by for i in frame_video: out.write(i)????\n",
    "    \n",
    "    \n",
    "        ####QUESTIONS???####\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    out.release()\n",
    "    #officially 'releases' this video/movie to the public after its been created\n",
    "\n",
    "#So we're cycling thru the images, defining the size of each, which will be its own frame, and these images/frames\n",
    "#will be pieced together in/as a movie reel and output/written according to the specs as its formatted/converted to\n",
    "#movie format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_training_flip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cr/9gwdd0wn43d9d1db4xfhyvpc0000gn/T/ipykernel_92516/932098186.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mimg_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_training_flip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Read the training not flip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_training_flip' is not defined"
     ]
    }
   ],
   "source": [
    "# Read and preprocess the training data\n",
    "\n",
    "#so in his example/template, he's classifying diff scenarios - pages to flip and pages that shouldn't be flipped\n",
    "#what am I doing?  well, i guess in a way, I'M deciding whether to FLIP or not too.... A PROPERTY!!!\n",
    "#so that means I needa manually sort thru each set of pics and train it based on which houses I want renovated and\n",
    "#which ones I want as is\n",
    "#so gotta fine/re-tune the objective/purpose/goal here. cuz we could make it like that^, or we could do more closely\n",
    "#to what we were initially thinking, which is more like the Big Mountain project, and is what the creators of the\n",
    "#parent dataset project i'm using did - just take ALL the data, split randomly, and use those to train factors to come\n",
    "#up with the predicted price? oh but then we'd need text data - we'd have to have baseline prices\n",
    "#i'm tryna remember how we did it in Big Mountain - we had prices, and then i think: found the most important factors\n",
    "#that influenced the prices, and then accordingly used those to PREDICT what prices SHOULD be, based off what everyone\n",
    "#else was doing. Similarly, this Ahmed/Moustafa project was for HOUSE PRICE ESTIMATION based off images AND text/numerical\n",
    "#data both!\n",
    "\n",
    "#but I was told it gets tricky when you try to do both, even though that makes more sense - that's obviously how a human\n",
    "#would work, but to do that w/ computers takes advanced neural networking / deep learning etc and I'm not quite at that\n",
    "#point yet. So, we'll simplify and use images alone! that's why this makes sense that we would have to do CATEGORIES then-\n",
    "#because this is traditionally used as/or at least one very common/famous example/use of this is for CLASSIFICATION!\n",
    "#so basically, image/face recognition like Apple's FaceID & furry friends & laying out all the 'people' in/from your pictures\n",
    "#so you can quickly go to the pictures w/ them! and of course the first famous one that i knew - Facebook's facial recognition\n",
    "#for tagging suggestions where they look at your pictures, look at the faces in them, and cross ref w/ your friends'\n",
    "#pictures to get matches (lol what if your friend only has like a dog / only ever has dogs as their profile pic - no humans\n",
    "#so that anytime you have a dog in your pic it suggests that it's that person! >v<)\n",
    "#so yeah, then if we're only doing images, we don't have any prices to train it on, we can only pre-break it up and feed\n",
    "#it what's what - so what properties should be:\n",
    "#Renovated/Flipped - ones that are in poorer condition\n",
    "\n",
    "\n",
    "#would be great to learn how to do both images and text so we can look at/consider other factors like neighborhood etc!\n",
    "#and look at the asking price, come up with a predicted ACTUAL selling price, and then estimate calculations for the cost\n",
    "#of renovating and the potential PROFIT MARK-UP / PRICE WE CAN SELL IT AT ONCE WE RENOVATE!! and also give the price\n",
    "#we can get if we decide to RENT IT OUT!!! and then of course provide an accompanying report of like a cash flow/RoR\n",
    "#analysis!!! It may be a slightly different algorithm for rental properties as far as like what costs go into it cuz\n",
    "#may not spend on the same things cuz renovating for different purposes/diff audience. completely diff cash recovery/return\n",
    "#method. so the cash flow obviously will look completely diff, diff costs etc\n",
    "#so in that ideal scenario, the categories could be like: Renovate-Flip To Sell, Renovate-Flip To Rent,\n",
    "#Buy-As-Is=>>TURNKEY To Rent, or Pass\n",
    "#(could even have it look for rental properties you wanna renovate and SELL and not rent out yourself!)\n",
    "#factor in whether to pay cash or finance\n",
    "\n",
    "#but for now, we'll keep it / start off simple and just classify as Renovate/Flip or Pass/Not Flip\n",
    "\n",
    "\n",
    "#HMMMmmmm, but now that i look more closely at this dataset I have, these are almost all NICE houses that are ready to\n",
    "#go and wouldn't be candidates for flipping. and these are all sfh's i believe so may not have alot of options for renting\n",
    "#either, unless there are some small ones. but again, identifying / subclassifying for renting is outside of the\n",
    "#scope of this most likely since we don't have text data to tell us what's a multi fam vs. a condo etc and don't have\n",
    "#the square footage and 'num of dwellings' to support that\n",
    "\n",
    "#################################################################################################################################\n",
    "#SO - that may mean that i need to MANUALLY collect/develop my OWN database of images!!!! both for training and testing purposes\n",
    "#################################################################################################################################\n",
    "\n",
    "\n",
    "####QUESTION!!!####\n",
    "#In image machine learning, do we also train it on what does NOT constitute a category, so that it doesn't get confused\n",
    "#by other things/special/rare circumstances it might occur and knows how to handle it?\n",
    "#for example, with real estate, in general we'll teach it to look for stuff that's outdated, based on style and color\n",
    "#(and oftentimes even low image quality alone will indicate a bad situation i.e. a GOOD opportunity to renovate/flip\n",
    "#but i guess it wouldn't matter too much cuz would still needa base on elements of image). also ARRANGEMENT - like if\n",
    "#things are messy/in disarray. but what about if it encounters stuff that, technically, yes, is outdated, but it's in\n",
    "#SEVERE disrepair, abandoned! how will / do we teach it to NOT classify those as investment opportunities / flips but\n",
    "#rather as Do Not Buy's!\n",
    "\n",
    "\n",
    "\n",
    "img_training = image_preprocessing(path = path_training_flip)\n",
    "\n",
    "# Read the training not flip\n",
    "\n",
    "img_training_notflip = image_preprocessing(path = path_training_notflip)\n",
    "\n",
    "# Read the test flip\n",
    "\n",
    "img_testing_flip = image_preprocessing(path = path_testing_flip)\n",
    "\n",
    "# Read the test not flip\n",
    "\n",
    "img_testing_notflip = image_preprocessing(path = path_testing_notflip)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the problem\n",
    "\n",
    "y_train_flip = [1 for i in range(0, len(img_training_flip))]\n",
    "\n",
    "y_train_notflip = [0 for i in range(0, len(img_training_notflip))]\n",
    "\n",
    "y_test_flip = [1 for i in range(0, len(img_testing_flip))]\n",
    "\n",
    "y_test_notflip = [0 for i in range(0, len(img_testing_notflip))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the video for the training flip\n",
    "\n",
    "video_creator(path = path_training_flip, pathIn = 'training_flip.avi', time = len(path_training_flip), fps = 1)\n",
    "\n",
    "# Create the video for the training not flip\n",
    "\n",
    "video_creator(path = path_training_notflip, pathIn = 'training_notflip.avi', time = len(img_training_notflip), fps = 1)\n",
    "\n",
    "# Create the video for the test flip\n",
    "\n",
    "video_creator(path = path_testing_flip, pathIn = 'test_flip.avi', time = len(img_testing_flip), fps = 1)\n",
    "\n",
    "# Create the video for the test not flip\n",
    "\n",
    "video_creator(path = path_testing_notflip, pathIn = 'test_notflip.avi', time = len(img_testing_notflip), fps = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X_train, X_test, y_train and y_test for analysis\n",
    "\n",
    "X_train = np.concatenate((img_training_flip, img_training_notflip), axis = 0)\n",
    "\n",
    "X_test = np.concatenate((img_testing_flip, img_testing_notflip), axis = 0)\n",
    "\n",
    "y_train = np.append(y_train_flip, y_train_notflip)\n",
    "\n",
    "y_test = np.append(y_test_flip, y_test_notflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if the shapes matches between the X_trian and y_train and the X_test and y_test\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new array that will have the original arrays (labels and values) but they will be shuffled. \n",
    "\n",
    "# Create the array for the train data set\n",
    "\n",
    "X_train_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_train):\n",
    "    # The new array would be the array containing the image plus its label \n",
    "    new_array = (j, y_train[i])\n",
    "    # Append the values to the array that will be shuffled\n",
    "    X_train_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays\n",
    "X_train_shuffle = np.array(X_train_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array for the test data set\n",
    "\n",
    "X_test_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_test):\n",
    "    # The new array would be the array containing the image plus its label \n",
    "    new_array = (j, y_test[i])\n",
    "    # Append the values to the array that will be shuffled\n",
    "    X_test_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays  \n",
    "X_test_shuffle = np.array(X_test_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the random shuffle to make the train and test with no specific order\n",
    "\n",
    "np.random.shuffle(X_train_shuffle)\n",
    "\n",
    "np.random.shuffle(X_test_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate between the X_train and y_train to fit the model\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Start a for loop into the X_train_shuffle\n",
    "for i in X_train_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_train.append(value)\n",
    "    y_train.append(label)\n",
    "\n",
    "# Divide between X_train and y_train to run model\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the test data set\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Start a for loop into the X_test_shuffle\n",
    "for i in X_test_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_test.append(value)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure labels are same than the first shapes\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function with the neural networks\n",
    "\n",
    "def neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer='he_uniform', \n",
    "                     padding = 'same', input_shape=(100, 70, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "\n",
    "model = neural_network()\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get them into 0 and 1 values\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "# Start a for loop to iterate over the predictions array\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:\n",
    "        binary_values.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix to evaluate the model\n",
    "\n",
    "cm = confusion_matrix(y_test, binary_values)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['not_flip','flip'])\n",
    "cmd.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Classification report to get the precision, recall, f1-score\n",
    "\n",
    "print(classification_report(y_test, binary_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### I was able to create a model with 0.99 accuracy if a page needs to whether be flipped or not by using deep learning and doing the necessary data preprocessing such as making all the pages the same size, adjusting bright, adding nose, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using pickle\n",
    "\n",
    "model_classifier = model.save('flip_page_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the training video by concatenating the flips with the not flips for train and test data sets.\n",
    "\n",
    "#Establish the train video\n",
    "\n",
    "video_1 = VideoFileClip('training_flip.avi')\n",
    "video_2 = VideoFileClip('training_notflip.avi')\n",
    "\n",
    "training_video = concatenate_videoclips([video_1, video_2])\n",
    "\n",
    "training_video.write_videofile('training_video.avi', codec = 'rawvideo')\n",
    "training_video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the test video\n",
    "\n",
    "video_3 = VideoFileClip('test_flip.avi')\n",
    "video_4 = VideoFileClip('test_notflip.avi')\n",
    "\n",
    "test_video= concatenate_videoclips([video_3, video_4])\n",
    "\n",
    "test_video.write_videofile('test_video.avi', codec = 'rawvideo')\n",
    "test_video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the frames for the training video\n",
    "\n",
    "count = 0\n",
    "\n",
    "videoFile = 'training_video.avi'\n",
    "# Capturing the video from the given path\n",
    "cap = cv2.VideoCapture(videoFile)   \n",
    "# Establish Frame rate\n",
    "frameRate = cap.get(5) \n",
    "\n",
    "x=1\n",
    "\n",
    "filenames_train = []\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True): \n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"frame%d.jpg\" % count;count+=1\n",
    "        filenames_train.append(filename)\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the frames from the test video\n",
    "\n",
    "count = 0\n",
    "\n",
    "videoFile = 'test_video.avi'\n",
    "# Capturing the video from the given path\n",
    "cap = cv2.VideoCapture(videoFile)   \n",
    "# Establish Frame rate\n",
    "frameRate = cap.get(5) \n",
    "\n",
    "x=1\n",
    "\n",
    "filenames_test = []\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"frame%d.jpg\" % count;count+=1\n",
    "        filenames_test.append(filename)\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the y_train flip and not flip into the same array\n",
    "\n",
    "y_train = np.append(np.array(y_train_flip), np.array(y_train_notflip))\n",
    "\n",
    "# Append the y_test flip and not flip into the same array\n",
    "\n",
    "y_test = np.append(np.array(y_test_flip), np.array(y_test_notflip))\n",
    "\n",
    "# Create the data frame that will show the frameID and the class for the train data\n",
    "\n",
    "data_train = pd.DataFrame({'frameID': filenames_train, 'flip': y_train})\n",
    "\n",
    "# Create the data frame that will show the frameID and the class for the test data\n",
    "\n",
    "data_test = pd.DataFrame({'frameID': filenames_test, 'flip': y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly change the data order and reset the index\n",
    "\n",
    "data_train = shuffle(data_train).reset_index(drop = True)\n",
    "\n",
    "data_test = shuffle(data_test).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array\n",
    "\n",
    "X_train = []   \n",
    "\n",
    "# Loop through the frameID column and store every frame in X\n",
    "for img_name in data_train.frameID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    X_train.append(img)  \n",
    "    \n",
    "# Convert the list to an array\n",
    "X_train = np.array(X_train)    \n",
    "\n",
    "# Define the y_train\n",
    "\n",
    "y_train = data_train['flip'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array\n",
    "\n",
    "X_test = []   \n",
    "\n",
    "# Loop through the frameID column and store every frame in X\n",
    "for img_name in data_test.frameID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    X_test.append(img)  \n",
    "    \n",
    "# Convert the list to an array\n",
    "\n",
    "X_test = np.array(X_test) \n",
    "\n",
    "# Define the y_test\n",
    "\n",
    "y_test = data_test['flip'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do the preprocessing for each frame of image of the video\n",
    "\n",
    "def image_preprocessing_frame(data):\n",
    "    # Create an empty list to store all the preprocessed images\n",
    "    images = []\n",
    "    # Start by creating a for loop through all the path and make the preprocessing to each image\n",
    "    for i in data:\n",
    "        # Adjust the size so all iamges will have the same size\n",
    "        img = cv2.resize(i, dsize = (70,140), interpolation=cv2.INTER_CUBIC)\n",
    "        # Crop to remove part of the images I don't need for the modeling part\n",
    "        y,h,x,w = 0,100,0,70\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5\n",
    "        beta=0.5\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "        # Create the video\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the defined function to preprocess the data\n",
    "\n",
    "X_train = image_preprocessing_frame(data = X_train)\n",
    "\n",
    "X_test = image_preprocessing_frame(data = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include_top=False to remove the top layer and a base model\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 70, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list into arrays\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make predictions using this model for X_train and X_valid, get the features, and then use those \n",
    "# features to retrain the model.\n",
    "\n",
    "X_train = base_model.predict(X_train)\n",
    "\n",
    "X_test = base_model.predict(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering the data\n",
    "\n",
    "X_train = X_train/X_train.max()\n",
    "\n",
    "X_test = X_test/X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network to use to predict if the frame of the video is flip or not flip\n",
    "\n",
    "def model_neural():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer='he_uniform', \n",
    "                     padding = 'same', input_shape=(3, 2, 512)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function with the created model\n",
    "\n",
    "model = model_neural()\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get them into 0 and 1 values\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "# Start a for loop to iterate over the predictions array\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:\n",
    "        binary_values.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix to evaluate the model\n",
    "\n",
    "cm = confusion_matrix(y_test, binary_values)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['not_flip','flip'])\n",
    "cmd.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Classification report to get the precision, recall, f1-score\n",
    "\n",
    "print(classification_report(y_test, binary_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
