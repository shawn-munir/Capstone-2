{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Profit Opportunity Identification For Improvement\n",
    "\n",
    "## Step_1: Data Wrangling\n",
    "\n",
    "Normally, at least at this level of training, data wrangling involves cleaning up text. But we are attempting something more challenging and sophisticated - wrangling *images*. In fact, this project will involve *only* images and no text/numerical data\n",
    "\n",
    "To do so, we start with this template with packages and tools such as TensorFlow (paired with keras), cv2 & glob\n",
    "\n",
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2 #to read images\n",
    "import glob #to tell it what kind of files to read within the filepath, in this case .jpg's\n",
    "import skvideo.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to divide up these imports because taking forever so need to see what the holdup is\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image   # for preprocessing the images\n",
    "from tensorflow.keras.utils import to_categorical #np_utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Activation, MaxPooling2D, Conv2D, InputLayer)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "#oh nice!!! finally got all of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import double, jit, njit, vectorize\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, ConfusionMatrixDisplay)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle\n",
    "\n",
    "from skimage.transform import resize   # for resizing images\n",
    "\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the path to read all of the images \n",
    "\n",
    "#flip/not flip was the images training on which ones are examples of \"pages to flip or not to flip\" - that was the nature\n",
    "#of this machine learning application\n",
    "\n",
    "# path_training_flip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/training/flip/*.jpg')\n",
    "# path_training_notflip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/training/notflip/*.jpg')\n",
    "# path_testing_flip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/testing/flip/*.jpg')\n",
    "# path_testing_notflip = glob.glob('/Users/arnaldofolder/Documents/Apziva/Fourth Project/images/testing/notflip/*.jpg')\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "#Will use 80/20 split for training/testing, following Ahmed/Moustafa model study\n",
    "\n",
    "#Training\n",
    "path_training_flip = glob.glob('/Users/deens/OneDrive/Documents/Career/DataScienceMachineLearning/Tools/git/springboard/springboard-data-science/Capstone-2/Houses Dataset/Training Set/Flip/*.jpg')\n",
    "path_training_noflip = glob.glob('/Users/deens/OneDrive/Documents/Career/DataScienceMachineLearning/Tools/git/springboard/springboard-data-science/Capstone-2/Houses Dataset/Training Set/Dont Flip/*.jpg')\n",
    "\n",
    "#lol wow so that last slash at the very end actually DOES matter!! it didn't work without it - was getting 'empty set' error\n",
    "#but it won't tell you that here for some reason\n",
    "\n",
    "#Testing\n",
    "path_testing_flip = glob.glob('/Users/deens/OneDrive/Documents/Career/DataScienceMachineLearning/Tools/git/springboard/springboard-data-science/Capstone-2/Houses Dataset/Testing Set/Flip/*.jpg')\n",
    "path_testing_noflip = glob.glob('/Users/deens/OneDrive/Documents/Career/DataScienceMachineLearning/Tools/git/springboard/springboard-data-science/Capstone-2/Houses Dataset/Testing Set/Dont Flip/*.jpg')\n",
    "\n",
    "#so what glob does is look for all files/filepaths that follow/contain a specified pattern, using the *wild card,\n",
    "#so here *.jpg\n",
    "\n",
    "\n",
    "#so just thought of something - the point for testing is IT'S supposed to tell us whether it thinks it should be a flip/\n",
    "#not flip... so i was like then why are WE giving it the answer right off the bat!? but the thing is it doesn't realize\n",
    "#we're telling it the answer. we're only doing this for our OWN purposes for organization to make it easy for ourselves/\n",
    "#to see/compare ITS answers to the RIGHT answers!\n",
    "#wait actually so... how does it work to tell you the 'accuracy' of the prediction at the end?\n",
    "#like cuz you label the flip/not flip. so like is it just taking those images and tryna slap a label on and then\n",
    "#comparing to your label? how does it not cheat/get bias? how does it do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will do all the preprocessing for each image to be ready for modeling\n",
    "\n",
    "def image_preprocessing(path):   #so with this function we'll define the filepath. in fact, that's the ONLY argument\n",
    "                                 #image_preprocessing is just the name we've given to this function\n",
    "    # Create an empty list to store all the preprocessed images\n",
    "    images = []\n",
    "    # Start by creating a for loop through all the path images and make the preprocessing to each image\n",
    "    #so it'll go thru each image / file in the filepath/folder one by one, but remember the path uses GLOB so it's only\n",
    "    #getting .jpg's in our case since that's what we defined/wildcarded/told it to do. there should only be .jpg's in\n",
    "    #there anyway\n",
    "    for i in path:\n",
    "        # Firstly read the image (using the cv2)\n",
    "        img = cv2.imread(i)\n",
    "        # Adjust the size so all iamges will have the same size\n",
    "        img = cv2.resize(img, dsize = (70,140), interpolation=cv2.INTER_CUBIC)\n",
    "        #interpolation tells it how to \"create new pixels\" to make the image look smoother as you make it bigger,\n",
    "        #rather than just stretch the original pixels and risking \"pixely\" images\n",
    "        #bicubic is the smoothest cuz it's curved/polynomial\n",
    "        # Crop to remove excess of the images we don't need for modeling, like around the border/near the edges. unnecessary noise\n",
    "        #watermarks/text etc\n",
    "        \n",
    "        #okay so we just set the pic above to be 70 width x 140 height\n",
    "        #so now we're cropping it to be 70 (keep all width) x 100 height, so chopping. but where does the counter\n",
    "        #start from? i guess/assuming top left is (0,0)? this comes into play in the below\n",
    "        y,h,x,w = 0,100,0,70 #figured it out below!! y,x are the STARTING pixel location height, width address/coords\n",
    "        #to START trim at / KEEP picture at, then h,w is THE ACTUAL DESIRED LENGTH OF THE PICTURE!!!\n",
    "        img = img[y:y+h, x:x+w] #if y & x are 0, why not just do img[y:h, x:w]?\n",
    "        #i guess we do this IN CASE, which actually may be MOST cases, that we're NOT NECESSARILY STARTING FROM THE\n",
    "        #EDGE!!! like this is setting the pixels we wanna KEEP, so let's say you wanted to trim centered/evenly\n",
    "        #so that you take 20 pixels off the top and 20 pixels off the bottom, then it'd be: img = img[20:100+20,...]\n",
    "        #so since/if (0,0) is top left, then this example of 0,100,0,70 will keep only TOP 100 pixels and\n",
    "        #chop off BOTTOM 40!!\n",
    "        \n",
    "        #SO THE y & x ARE THE *STARTING* y,x pixel point you wanna begin crop at/start keeping at!!!\n",
    "        #then of course the h & w are actual height and width in pixels, so to get the right crop you gotta treat like\n",
    "        #this,- algebraically/addressly/plotly, so that, say you want the new cropped pic to be only the MIDDLE 100\n",
    "        #vertical pixels in heighth, out of 140 original, then you gotta START AT 20, then \"ADD\" 100 to represent the\n",
    "        #desired image size, which'll take it to 120,-> THE DESIRED END LENGTH PIXEL LOCATION!!!\n",
    "        \n",
    "        ####QUESTION???####\n",
    "        \n",
    "        \n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5 #contrast/gain\n",
    "        beta=0.5  #brightness/bias\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        #addWeighted helps to blend/transition two images together, by specifiying respective weights, like how visible\n",
    "        #or transparent one is\n",
    "        #the np.zeros thing is the 2nd image?? it's creating an image of 0 size and 0 dtype? what's 0 dtype?\n",
    "        #is this just a way of creating a 'blank' image to make it\n",
    "        #then what is 0?? the template is:\n",
    "        #cv.addWeighted(image1, alpha, image2, beta, gamma[, dst[, dtype]]) #note this is cv and not cv2... any difference?\n",
    "        #this is confusing cuz the *0* is in the BETA spot and BETA is in the GAMMA spot! mistake??\n",
    "        #also more confusing cuz alpha & beta here are the respective WEIGHTS to give to each image\n",
    "        #whereas above it had to do w/ contrast & brightness?\n",
    "        #and also, it seems like for image2 we're creating a 'blank' image, which we're further giving 0 weight...\n",
    "        #which poses the question - why even bother doing a blend if we're not blending?? we're only using the first\n",
    "        #image by itself! well maybe we're blending an image with a blank image to attempt to create a picture with a\n",
    "        #'transparent' background, like those png images! to remove any excess / nonessential background noise around the\n",
    "        #main part / meat of the picture?\n",
    "        #but also, shouldn't alpha & beta be on a scale of 0 to 1 and sum to 1?? Or actually, I guess you could look at\n",
    "        #it as each one can be an independent percentage transparency of its full resolution, so they can both be 1\n",
    "        #for example and so don't necess have to add up to 1... BUT they DO both still need to be some RATIO / can't\n",
    "        #be more than 1! but alpha is 1.5??\n",
    "        \n",
    "        \n",
    "        \n",
    "        ####QUESTIONS???!!!####\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Normalize the images to be black and white by reverting the images and then dividing by 255.0\n",
    "        \n",
    "        #this would be an important step in applications where color is irrelevant and it's just noise and you wanna focus\n",
    "        #on the features. but in our application, REAL ESTATE, color is definitely very important. like if the floors are\n",
    "        #modern gray, that property will def have more value\n",
    "        \n",
    "        #img = cv2.bitwise_not(img)  #can look up this function later\n",
    "        img = img/255               #and can look up why divide by 255 >> TO REDUCE ALL THE IMAGES TO BETWEEN 0 & 1!!!\n",
    "        #THAT'S CUZ THE COLOR RATING/VALUE RANGE IS FROM 0 TO 255!!!\n",
    "        #ORIGINALLY I THOUGHT THIS WAS DIVIDING BY 255 TO GET EVERYTHING IN GRAYSCALE LOL SO I COMMENTED OUT!!\n",
    "\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "        # Create the video\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images\n",
    "\n",
    "#okay so overall, this is iterating thru our images in our path folder and reshaping/resizing/recoloring them and\n",
    "#tryna crop out the background noise as much as possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to create a video from images\n",
    "\n",
    "#uhhh wait, why are we creating a video?? do we need that for our purposes? it's making like a slideshow?\n",
    "\n",
    "def video_creator(path, pathIn, time, fps): #so the user is specifying: (1) the path to get the images from,\n",
    "    frame_video = [] #(2) the filename to name the resultant video (will go to/in same path), (3) the length of\n",
    "    for i in path: #the video?? (what units??) (don't see this used tho!!),& (4) the frames per second\n",
    "        \n",
    "        \n",
    "         ####QUESTION???####\n",
    "        \n",
    "        \n",
    "        img = cv2.imread(i)\n",
    "        height, width, layers = img.shape   #what's layers? and i guess all 3 are defined in img.shape -- but where is\n",
    "        size = (width, height)              #that defined?? in above use we seem to have made it 0/0s, but we never\n",
    "        frame_video.append(img)             #stored that as like a variable?? but i guess this is just saying that\n",
    "    \n",
    "    out = cv2.VideoWriter(pathIn, cv2.VideoWriter_fourcc(*'mp4v'), fps, size) #make these 3 things equal to WHATEVER\n",
    "    #out is just the output filepath & specs themselves but NOT\n",
    "    #the act of WRITING the output itself?? that's what the below iterator\n",
    "    #is for??\n",
    "    \n",
    "    \n",
    "        ####QUESTIONS???####\n",
    "\n",
    "    \n",
    "    for i in range(len(path)):                                                #img.shape evaluates to - like it's just\n",
    "        out.write(frame_video[i])                                             #a function that can be called\n",
    "    \n",
    "        #why do we do for i in range(len(path)) instead of just for i in path, like we did above??\n",
    "        #it's a way of iterative printing by calling the index rather than simply calling the elements one by one in order\n",
    "        #as it standard/ly does. but isn't calling by index what it's inherently/implicitly/automatically doing??\n",
    "        #what would the length of path be anyway? cuz if path is just a text string for/giving the filepath/directory,\n",
    "        #then isn't len(path) just literally the character count? i.e. number of characters in the path name?\n",
    "        #but maybe this is doing the ACTUAL writing of each image to the out/put path, defined above, in accordance with\n",
    "        #the specs defined in that out/put function, so it's iterating thru each image in the path and making it into\n",
    "        #the final product output video?\n",
    "        #so i think this process is IV/FOUR/4 steps:\n",
    "        #(1) we cycle thru each image, resize,etc it\n",
    "        #(2) append each of theses images to a collective/aggregated 'reel', which is just a collection of images,\n",
    "        #just like how a pdf is a collection of pages, but it's not a MOVIE yet cuz it hasn't been formatted as one/to move!\n",
    "        #that's what step 4 is for! the conversion/formatting. but before that, before the actual piecemeal conversion,\n",
    "        #we gotta:\n",
    "        #(3) set up the specs for the final output video, like as far as format type, fps, size, etc\n",
    "        #(4) write each one of those pics/frames in the collective/tion to the final output conversion/formatting function \"out\"\n",
    "        #for formatting/conversion in accordance w/ the defined/outlined specs to become a / the final video\n",
    "        #product output!\n",
    "        #again tho, couldn't we just have / simply achieved this by for i in frame_video: out.write(i)????\n",
    "    \n",
    "    \n",
    "        ####QUESTIONS???####\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    out.release()\n",
    "    #officially 'releases' this video/movie to the public after its been created\n",
    "\n",
    "#So we're cycling thru the images, defining the size of each, which will be its own frame, and these images/frames\n",
    "#will be pieced together in/as a movie reel and output/written according to the specs as its formatted/converted to\n",
    "#movie format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the training data\n",
    "\n",
    "#so in his example/template, he's classifying diff scenarios - pages to flip and pages that shouldn't be flipped\n",
    "#what am I doing?  well, i guess in a way, I'M deciding whether to FLIP or not too.... A PROPERTY!!!\n",
    "#so that means I needa manually sort thru each set of pics and train it based on which houses I want renovated and\n",
    "#which ones I want as is\n",
    "#so gotta fine/re-tune the objective/purpose/goal here. cuz we could make it like that^, or we could do more closely\n",
    "#to what we were initially thinking, which is more like the Big Mountain project, and is what the creators of the\n",
    "#parent dataset project i'm using did - just take ALL the data, split randomly, and use those to train factors to come\n",
    "#up with the predicted price? oh but then we'd need text data - we'd have to have baseline prices\n",
    "#i'm tryna remember how we did it in Big Mountain - we had prices, and then i think: found the most important factors\n",
    "#that influenced the prices, and then accordingly used those to PREDICT what prices SHOULD be, based off what everyone\n",
    "#else was doing. Similarly, this Ahmed/Moustafa project was for HOUSE PRICE ESTIMATION based off images AND text/numerical\n",
    "#data both!\n",
    "\n",
    "#but I was told it gets tricky when you try to do both, even though that makes more sense - that's obviously how a human\n",
    "#would work, but to do that w/ computers takes advanced neural networking / deep learning etc and I'm not quite at that\n",
    "#point yet. So, we'll simplify and use images alone! that's why this makes sense that we would have to do CATEGORIES then-\n",
    "#because this is traditionally used as/or at least one very common/famous example/use of this is for CLASSIFICATION!\n",
    "#so basically, image/face recognition like Apple's FaceID & furry friends & laying out all the 'people' in/from your pictures\n",
    "#so you can quickly go to the pictures w/ them! and of course the first famous one that i knew - Facebook's facial recognition\n",
    "#for tagging suggestions where they look at your pictures, look at the faces in them, and cross ref w/ your friends'\n",
    "#pictures to get matches (lol what if your friend only has like a dog / only ever has dogs as their profile pic - no humans\n",
    "#so that anytime you have a dog in your pic it suggests that it's that person! >v<)\n",
    "#so yeah, then if we're only doing images, we don't have any prices to train it on, we can only pre-break it up and feed\n",
    "#it what's what - so what properties should be:\n",
    "#Renovated/Flipped - ones that are in poorer condition\n",
    "\n",
    "\n",
    "#would be great to learn how to do both images and text so we can look at/consider other factors like neighborhood etc!\n",
    "#and look at the asking price, come up with a predicted ACTUAL selling price, and then estimate calculations for the cost\n",
    "#of renovating and the potential PROFIT MARK-UP / PRICE WE CAN SELL IT AT ONCE WE RENOVATE!! and also give the price\n",
    "#we can get if we decide to RENT IT OUT!!! and then of course provide an accompanying report of like a cash flow/RoR\n",
    "#analysis!!! It may be a slightly different algorithm for rental properties as far as like what costs go into it cuz\n",
    "#may not spend on the same things cuz renovating for different purposes/diff audience. completely diff cash recovery/return\n",
    "#method. so the cash flow obviously will look completely diff, diff costs etc\n",
    "#so in that ideal scenario, the categories could be like: Renovate-Flip To Sell, Renovate-Flip To Rent,\n",
    "#Buy-As-Is=>>TURNKEY To Rent, or Pass\n",
    "#(could even have it look for rental properties you wanna renovate and SELL and not rent out yourself!)\n",
    "#factor in whether to pay cash or finance\n",
    "\n",
    "#but for now, we'll keep it / start off simple and just classify as Renovate/Flip or Pass/Not Flip\n",
    "\n",
    "\n",
    "#HMMMmmmm, but now that i look more closely at this dataset I have, these are almost all NICE houses that are ready to\n",
    "#go and wouldn't be candidates for flipping. and these are all sfh's i believe so may not have alot of options for renting\n",
    "#either, unless there are some small ones. but again, identifying / subclassifying for renting is outside of the\n",
    "#scope of this most likely since we don't have text data to tell us what's a multi fam vs. a condo etc and don't have\n",
    "#the square footage and 'num of dwellings' to support that\n",
    "\n",
    "#################################################################################################################################\n",
    "#SO - that may mean that i need to MANUALLY collect/develop/compile my OWN database of images!!!! both for training and testing purposes\n",
    "#################################################################################################################################\n",
    "\n",
    "\n",
    "####QUESTION!!!####\n",
    "#In image machine learning, do we also train it on what does NOT constitute a category, so that it doesn't get confused\n",
    "#by other things/special/rare circumstances it might occur and knows how to handle it?\n",
    "#YES!! that's what the NOT_FLIP is for!!\n",
    "#for example, with real estate, in general we'll teach it to look for stuff that's outdated, based on style and color\n",
    "#(and oftentimes even low image quality alone will indicate a bad situation i.e. a GOOD opportunity to renovate/flip\n",
    "#but i guess it wouldn't matter too much cuz would still needa base on elements of image). also ARRANGEMENT - like if\n",
    "#things are messy/in disarray. but what about if it encounters stuff that, technically, yes, is outdated, but it's in\n",
    "#SEVERE disrepair, abandoned! how will / do we teach it to NOT classify those as investment opportunities / flips but\n",
    "#rather as Do Not Buy's!\n",
    "#so we may need to stick to just one kind of property, i.e. single family homes, and not do like condos or townhomes\n",
    "#if we're factoring in the EXTERIOR of buildings cuz that would proabably throw it off and would be better to keep those\n",
    "#all separate at first and then combine / aggregate later\n",
    "#BUT - we COULD use ONLY interior pictures, in which case it wouldn't matter if it's a SFH, townhome or condo - and\n",
    "#actually that makes more sense to only use interior bc that's the MAIN BASIS for deciding whether something is a good\n",
    "#flip or not because that's the MAIN DRIVER of price in people's minds. cuz think about it - in a condo highrise or\n",
    "#townhome village, - the exterior is shared/identical! so what sets them apart that can drastically alter price!?\n",
    "#(other than possibly view) >> interior design!!!\n",
    "\n",
    "\n",
    "#img_training = image_preprocessing(path = path_training_flip)\n",
    "#Don't need to write path since that's the only arg / in general don't need if following/aligned w/ order\n",
    "#deleted the remainder originals - all like this\n",
    "\n",
    "# Read the training not flip\n",
    "\n",
    "img_training_flip = image_preprocessing(path_training_flip)\n",
    "\n",
    "# Read the training not flip\n",
    "\n",
    "img_training_noflip = image_preprocessing(path_training_noflip)\n",
    "\n",
    "# Read the test flip\n",
    "\n",
    "img_testing_flip = image_preprocessing(path_testing_flip)\n",
    "\n",
    "# Read the test not flip\n",
    "\n",
    "img_testing_noflip = image_preprocessing(path_testing_noflip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the labels\n",
    "\n",
    "#so this will output 1 for EVERY element in the folder?? the label is just gonna be 1 for each element?\n",
    "#and what is the y representing anyway?\n",
    "#ohhh, wait, think i got it. so we wanna train this thing to know when to flip and when not to flip, so \"DO flip\"\n",
    "#is 1, aka TRUE!!! and don't is 0, seen below\n",
    "#so we're assigning a value of 1/True for EVERY item in the training\n",
    "#i have to look back at Big Mountain but i think we used labels when we were labeling the points as/w/ the state names\n",
    "#but here the label also represents the value\n",
    "\n",
    "#im a little confused on doing it for the testing set tho? again gotta look at Big Mountain but why are we assigning its\n",
    "#values? isn't the point that we're supposed to SEE how well it does, like it SHOULD result in 1's for flips and 0's\n",
    "#for nonflips... so maybe it's just gonna use these as benchmarks of what they SHOULD/'VE BEEN so we can see how the\n",
    "#actuals compare to these shoulds/ideals/theoreticals\n",
    "y_train_flip = [1 for i in range(0, len(img_training_flip))]\n",
    "\n",
    "y_train_noflip = [0 for i in range(0, len(img_training_noflip))]\n",
    "\n",
    "y_test_flip = [1 for i in range(0, len(img_testing_flip))]\n",
    "\n",
    "y_test_noflip = [0 for i in range(0, len(img_testing_noflip))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay so this uses the video_creator function we made above and supplies the needed arguments - the path (globbed)\n",
    "#containing the images, the name to name/call the resultant output file, the time - which they're making as just the\n",
    "#length of pictures in that folder - so is that in seconds then?? like i.e. one sec of video per photo?? and again,\n",
    "#from looking at the function def, can't tell where the time is getting factored in / where it's being used to build\n",
    "#the vid?\n",
    "\n",
    "\n",
    "#####QUESTION#####\n",
    "\n",
    "\n",
    "# # Create the video for the training flip\n",
    "\n",
    "# #don't need the arg labels but left alone to make clearer\n",
    "# video_creator(path = path_training_flip, pathIn = 'training_flip.avi', time = len(path_training_flip), fps = 1)\n",
    "\n",
    "# # Create the video for the training not flip\n",
    "\n",
    "# video_creator(path_training_noflip, pathIn = 'training_noflip.avi', time = len(img_training_noflip), fps = 1)\n",
    "\n",
    "# # Create the video for the test flip\n",
    "\n",
    "# video_creator(path_testing_flip, pathIn = 'test_flip.avi', time = len(img_testing_flip), fps = 1)\n",
    "\n",
    "# # Create the video for the test not flip\n",
    "\n",
    "# video_creator(path = path_testing_noflip, pathIn = 'test_noflip.avi', time = len(img_testing_noflip), fps = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############NOT SURE WHY THIS IS GIVING AN ERROR, BUT GONNA LEAVE THIS OUT ANYWAY SINCE WE'RE NOT USING VIDEO!\n",
    "#ERROR W/ SIZE PART OF FUNCTION: UnboundLocalError: local variable 'size' referenced before assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the X_train, X_test, y_train and y_test for analysis\n",
    "\n",
    "#hmm okay, so the X's are the images themselves, interesting! that's all there is to it - can summarize all of it\n",
    "#to simply that!\n",
    "#and the y's are what we saw before/above - 1's & 0's accordingly\n",
    "#and as we see, combine/unsegregate the flips/notflips into one! cuz together they still make up the training/testing sets\n",
    "\n",
    "#Note the difference b/w concat & append! concat defaults to stacking VERTICALLY -> axis = 0\n",
    "#and append defaults to FLATTENING COMPLETELY -> fully unraveling / stretching / connecting / daisy-chaining\n",
    "#ALL rows out to ONE SINGLE LONG ROW!!!\n",
    "\n",
    "#can do any kind of 3 stackings w/ either, but best to go w/ one who has what you're looking for as the default\n",
    "#so that's why we use concat for X, cuz we want those vertically stacked,\n",
    "#whereas we want our y's as one single long row\n",
    "#... wait WHY THO????\n",
    "#so for the X's, the flips & non-flips images will be two separate rows\n",
    "#for the y's, the flips & non-flips VALUES will be ONE SINGLE ROW!\n",
    "#question is - which direction are img_ & train_flip orientated?? which way does append naturally stack?\n",
    "#okay, so i did a basic append using an empty list, which is what it seems they did w/ image_processing, and it\n",
    "#APPENDS SIDEWAYS!!! horizontal/across/left-to-right. so then our hunch was right - concatting 2 X lists of images - those\n",
    "#lists are each sideways (as opposed to upright; they're 'supine' lol), so when we concat w/ axis=0, they'll be stacked\n",
    "#vertically, rowswise, so yes, each of these new X vars will be 2 rows, one flip one nonflip\n",
    "#okay and then checked for y's by doing a practice list comprehension and that ALSO created a sideways/-wise list!\n",
    "#thus, appending two y lists on axis None will make one longe list/row!!! and actually, didn't even need to practice\n",
    "#check cuz anytime doing axis None for either (concat or append), it'll first AUTOMATICALLY FLATTEN the composite\n",
    "#lists regardless of orientation! so that way it can just daisy-chain them together!\n",
    "#so not sure if there's a reason *WHY* both are oriented horizontal, but maybe that's the key point - maybe it's not\n",
    "#that they have to be oriented one way or the other, as long as they're oriented the SAME direction!\n",
    "\n",
    "\n",
    "#####QUESTION#####\n",
    "\n",
    "#X's are two rows\n",
    "X_train = np.concatenate((img_training_flip, img_training_noflip))#, axis = 0) #default axis is already 0!!!\n",
    "\n",
    "X_test = np.concatenate((img_testing_flip, img_testing_noflip))#, axis = 0) #can leave for emphasis/making it clear\n",
    "\n",
    "#y's are one long row\n",
    "y_train = np.append(y_train_flip, y_train_noflip)#, axis = None)\n",
    "\n",
    "y_test = np.append(y_test_flip, y_test_noflip)#, axis = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(462, 100, 70, 3)\n",
      "(462,)\n",
      "(123, 100, 70, 3)\n",
      "(123,)\n"
     ]
    }
   ],
   "source": [
    "# See if the shapes match between the X_train and y_train and the X_test and y_test\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#okay so 462 for training bc we got 253 items for flip & 209 for don't flip\n",
    "#BUT WHAT THE HECK IS THE 100, 70, 3????\n",
    "#isn't it only even possible for the shape to be 2-dimensional???\n",
    "#or at least, isn't that what it is here, since it's two rows??\n",
    "#and/but actually it's a little confusing cuz the two rows have diff num of columns\n",
    "#cuz first row is flip, which has more images than non-flip - so don't know if that causes issues?\n",
    "#not sure how Raghu's was divided between flip/not flip. but his also shows the 100, 70, 3\n",
    "#and it shows that for both training and testing???\n",
    "# and we got 123 items for testing bc we got 72 items for flip & 51 for don't flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [250, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [251, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [250, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[224, 233, 245],\n",
       "         [ 74, 127, 197],\n",
       "         [161, 202, 254],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[236, 242, 250],\n",
       "         [115, 164, 227],\n",
       "         [181, 221, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[236, 242, 248],\n",
       "         [169, 209, 255],\n",
       "         [ 83, 127, 191],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       [[[ 98, 142, 188],\n",
       "         [ 88, 125, 179],\n",
       "         [130, 160, 205],\n",
       "         ...,\n",
       "         [229, 230, 245],\n",
       "         [232, 241, 248],\n",
       "         [182, 193, 209]],\n",
       "\n",
       "        [[ 74, 116, 167],\n",
       "         [ 83, 130, 185],\n",
       "         [209, 248, 255],\n",
       "         ...,\n",
       "         [227, 229, 244],\n",
       "         [224, 233, 241],\n",
       "         [224, 235, 239]],\n",
       "\n",
       "        [[ 76, 118, 170],\n",
       "         [ 95, 134, 191],\n",
       "         [ 70, 107, 158],\n",
       "         ...,\n",
       "         [197, 199, 214],\n",
       "         [223, 232, 239],\n",
       "         [233, 245, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [251, 253, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [247, 247, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [238, 236, 250],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       [[[ 82, 146, 187],\n",
       "         [ 80, 145, 185],\n",
       "         [ 64, 128, 157],\n",
       "         ...,\n",
       "         [187, 255, 255],\n",
       "         [ 38,  56,  64],\n",
       "         [ 25,  28,  29]],\n",
       "\n",
       "        [[ 73, 143, 184],\n",
       "         [ 76, 146, 182],\n",
       "         [ 68, 131, 160],\n",
       "         ...,\n",
       "         [179, 255, 255],\n",
       "         [ 41,  59,  68],\n",
       "         [ 25,  26,  26]],\n",
       "\n",
       "        [[ 82, 154, 196],\n",
       "         [ 82, 145, 182],\n",
       "         [ 70, 133, 161],\n",
       "         ...,\n",
       "         [202, 255, 255],\n",
       "         [ 44,  61,  73],\n",
       "         [ 23,  23,  23]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 62,  65,  65],\n",
       "         [ 59,  65,  67],\n",
       "         [107, 110, 112],\n",
       "         ...,\n",
       "         [ 19,  83, 148],\n",
       "         [ 28,  44,  40],\n",
       "         [  8,   8,   8]],\n",
       "\n",
       "        [[ 61,  64,  65],\n",
       "         [ 56,  59,  59],\n",
       "         [101, 107, 109],\n",
       "         ...,\n",
       "         [ 28,  91, 155],\n",
       "         [ 37,  40,  38],\n",
       "         [  8,   8,   8]],\n",
       "\n",
       "        [[ 62,  65,  65],\n",
       "         [ 56,  59,  59],\n",
       "         [101, 107, 109],\n",
       "         ...,\n",
       "         [ 29,  92, 157],\n",
       "         [ 25,  40,  35],\n",
       "         [ 10,  10,  10]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[155, 220, 255],\n",
       "         [151, 215, 255],\n",
       "         [161, 224, 255],\n",
       "         ...,\n",
       "         [191, 239, 251],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[158, 223, 255],\n",
       "         [160, 224, 255],\n",
       "         [160, 223, 255],\n",
       "         ...,\n",
       "         [217, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[149, 221, 255],\n",
       "         [166, 229, 255],\n",
       "         [160, 223, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 82, 143, 172],\n",
       "         [149, 209, 241],\n",
       "         [182, 245, 255],\n",
       "         ...,\n",
       "         [152, 212, 255],\n",
       "         [255, 255, 255],\n",
       "         [215, 255, 255]],\n",
       "\n",
       "        [[181, 239, 255],\n",
       "         [191, 250, 255],\n",
       "         [188, 250, 255],\n",
       "         ...,\n",
       "         [238, 255, 255],\n",
       "         [248, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[191, 253, 255],\n",
       "         [184, 251, 255],\n",
       "         [185, 248, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[  8,   8,   8],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [ 95,  95,  95],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[103, 145, 176],\n",
       "         [134, 187, 223],\n",
       "         [106, 154, 182],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[115, 158, 190],\n",
       "         [110, 163, 199],\n",
       "         [106, 155, 184],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[113, 163, 193],\n",
       "         [113, 164, 200],\n",
       "         [ 94, 143, 172],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]]],\n",
       "\n",
       "\n",
       "       [[[ 76, 143, 209],\n",
       "         [ 76, 143, 209],\n",
       "         [ 82, 146, 211],\n",
       "         ...,\n",
       "         [143, 208, 255],\n",
       "         [143, 206, 255],\n",
       "         [142, 205, 255]],\n",
       "\n",
       "        [[ 67, 136, 208],\n",
       "         [ 70, 134, 208],\n",
       "         [ 71, 137, 208],\n",
       "         ...,\n",
       "         [127, 191, 250],\n",
       "         [128, 191, 244],\n",
       "         [136, 199, 251]],\n",
       "\n",
       "        [[ 64, 131, 208],\n",
       "         [ 70, 134, 211],\n",
       "         [ 65, 133, 209],\n",
       "         ...,\n",
       "         [128, 193, 251],\n",
       "         [130, 193, 245],\n",
       "         [142, 205, 255]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 73, 133, 196],\n",
       "         [ 77, 136, 202],\n",
       "         [128, 193, 255],\n",
       "         ...,\n",
       "         [ 89, 152, 215],\n",
       "         [ 92, 154, 212],\n",
       "         [ 89, 148, 206]],\n",
       "\n",
       "        [[ 74, 134, 197],\n",
       "         [ 76, 134, 200],\n",
       "         [127, 191, 255],\n",
       "         ...,\n",
       "         [ 92, 154, 212],\n",
       "         [ 92, 154, 212],\n",
       "         [ 89, 148, 206]],\n",
       "\n",
       "        [[ 76, 136, 199],\n",
       "         [ 76, 134, 200],\n",
       "         [125, 187, 255],\n",
       "         ...,\n",
       "         [ 97, 155, 212],\n",
       "         [ 92, 154, 212],\n",
       "         [ 91, 149, 208]]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oh yeah bruh! why don't we just see what these arrays even look like now that we have them/can!!\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm interesting... so that's why the shape is so complicated lol? the shape i guess isn't simply the rows & columns\n",
    "#w/ an array\n",
    "#like np.shape([[1, 3]]) >> (1, 2) bc there's one [list] with 2 elements\n",
    "#np.shape(np.array([(1, 2), (3, 4), (5, 6)]) >> (3,) bc it's 3 comma'd elements, but why no 1?....\n",
    "\n",
    "#okay so the numpy array shape is: (#dimensions, #elements in that dimension)\n",
    "#so each set of brackets is a dimension\n",
    "#so X(_train) has 462 images, so each one is a bracketed set, then each of those has a 70x3 list-of-lists, i.e. 70\n",
    "#bracketed sets, 3 elements each. this must have something to do w/ the image processing where this is how it\n",
    "#transcribes images as numerical data structures - must be based on the image size/number of pixels we specified which\n",
    "#is why it'd be the same in his data set and mine!!\n",
    "#prob gotta adjust the image size!!! based on what's appropriate given my image sizes\n",
    "#it could be that each set of 3 is THE COLOR PROFILE FOR ONE PIXEL!!! must be cuz you can see all the 255's which\n",
    "#represent black i believe which MUST BE WHY WE HE WAS DIVIDING BY 255 EARLIER TO MAKE GRAYSCALE!!!\n",
    "#AHHH OKAY YES!!! SO WE SPECIFIED IN THE IMAGE_PREPROCESSING TO MAKE THE HEIGHT AND WIDTH OF EACH PICTURE 100 X 70\n",
    "#(pixels i guess?) HEIGHT AND WIDTH!!!!!\n",
    "#okay so now it all makes sense - there's 462 images, and each of those is 100 x 70 pixels, and each pixel is composed\n",
    "#of/described by 3 color values. so there's 100 rows \n",
    "#you can see that there's 4 sets of brackets which may be why there's 4 elements in the shape?\n",
    "#OKAY ALHAMDULILLAH ALLAH GAVE ME THE ANSWER!!!:\n",
    "#each pixel is described by 3 color values, so there's 70 sets [brackets] of 3 across (width pixels),\n",
    "#and then 100 rows of those 70x3 sets of pixels going down (height pixels) -> which makes up the complete image for\n",
    "#ONE single images\n",
    "#and there's 462 images so then there's 462 sets of these 100 x 70 x 3 sets of pixels :D!!!!!\n",
    "#so i guess it's kinda irrelevant that there's 2 rows lol!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's do same for y\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay yeah so what we expected, and much easier to read/understand visually. it's simply ONE LONG single list\n",
    "#although it's a bit confusing cuz i thought the shape reps the dimensions and a dimension is signified by ONE / EACH\n",
    "#set of brackets and this is just one single?\n",
    "#interesting, so if there's just one single set of brackets then it'll be (462,) #second element 0?\n",
    "#so you CAN have just one set of brackets, AS LONG AS THERE'S NO INNER/SUB BRACKETS!!! like can't do np.array([1,2], [3,4])!\n",
    "#always gotta have an OUTER/WRAPPER pair/set of brackets\n",
    "#but for this case of just one set of brackets, the second is optional. if you DO do the second, then the shape'll be\n",
    "#(462,1)!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new array that will have the original arrays (labels and values) but they will be shuffled. \n",
    "\n",
    "# Create the array for the train data set\n",
    "# well this is just a(n empty) LIST, but we make it an array later at the end. but can we just\n",
    "#make it an/initialize an empty array NOW??\n",
    "\n",
    "X_train_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_train):\n",
    "    # The new array would be the array containing the image plus its label\n",
    "    \n",
    "    \n",
    "    #Hmm so enumerate prints the element along w/ its index number as a tuple pair\n",
    "    #but X_train is just the set horiz list of combined flip & non-flip iamges\n",
    "    #but this is saying it'll give the image + its 'label'. what's its label?\n",
    "    #we talked about / set labels for the y's above, as 1's or 0's\n",
    "    #but how do we tie those to these images?Okay so when we say label, we're talking\n",
    "    \n",
    "    #so enumerate(X_train) will give like [(0, 'img0'), (1, 'img1'), etc...]\n",
    "    #so the i,j in this refer to the index position and the image, respectively-> (index, image)\n",
    "    #so, this 'new_array' below makes (image, label)\n",
    "    #ahh okay. so y_train is comprised of an array of 1's for/from y_train_flip & an array of 0's from\n",
    "    #y_train_notflip. it's a single, combined, 1-row flattened array of all one's followed by all 0's\n",
    "    #it's number of elements was designed to be exactly matched to that of/ the number of images in its respective folder\n",
    "    #so y_train combined is the total number of training images, both flip and not flip\n",
    "    #now X_train is the combined set of images, but the first row is flips and second is NOT flips!\n",
    "    #similarly it's all the flips followed by all the nonflips\n",
    "    #SO the images and their corresponding values/labels are all slated to line up!! now just need to pair them together,\n",
    "    #which is exactly the point of enumerating/tupling!\n",
    "    #so new_array = (j, y_train[i]) pairs them off by taking j image sequentially from X_train and pairs it off w/\n",
    "    #the sequential value from/in y_train, which is lined up so it corresponds to it! so like [('img0', 1), ('img1', 1), etc...]\n",
    "    #did we need to do that though?\n",
    "    #couldn't we have just directly done:\n",
    "    #for i,j X_train, y_train: \n",
    "        #new_array = (X_train[i], y_train[j])???\n",
    "    #or even just concatted the 2 tables together?\n",
    "    #like wouldn't have been really easy to simply start w/ the X_ arrays and simply add a column to them of all 1's\n",
    "    #and all 0's as needed?\n",
    "    #ohhh, it could be because we want them as inseparable/interlocked PAIRS cuz we're gonna SHUFFLE THEM SO NEED THEM\n",
    "    #TO STAY INTACT!!!! can't do that w/ simple rows/columns/df format. this is more secure. cuz tuples are IMMUTABLE!!!\n",
    "    #but couldn't we spit out tuples directly w/ /create tuple pairs using ZIP, like simply:\n",
    "    #new = zip(X_train, y_train) --> so - this WOULD work if we were dealing with LISTS but NOT arrays! if you try this\n",
    "    #w/ arrays, it'll simply make ONE SINGLE TUPLE out of the WHOLE first array and WHOLE second array!\n",
    "    #i tried doing using a for loop but didn't work either:\n",
    "    #for x, y in keys, values:\n",
    "        #zip(keys[x], values[y])\n",
    "        #error: 'not enough values to unpack (expected 2, got 1)'...\n",
    "        #tried as/w inner loop/loop-w/in-loop, but that errored too\n",
    "        #also tried as lists instead of arrays, but that said 'TOO many values to unpack'! lol - but this'd be an\n",
    "        #unnecessarily long way to do it anyway for lists. if you got lists you got it easy!\n",
    "    #however, these 2 things probably COULD have been set up as lists instead of arrays\n",
    "    #BUT - i'm guessing there was probably a reason to set up as arrays, like maybe many of the other functions could\n",
    "    #only be done if np.arrays, like shuffling\n",
    "    #also, instead of appending new_array to X_train_shuffle,...\n",
    "    #WHY DIDN'T WE JUST DO IT DIRECTLY!!!\n",
    "    #like instead of new_array made it X_train_shuffle\n",
    "    #would we even had to have initalized it? cuz didn't initialize new_array!!! thus wouldn't need to/no need to\n",
    "    #do .append either if we did that!!!\n",
    "    #oh wait, note. so initially the i refers to the INDEX num of X_train, cuz that's the part of enumerate\n",
    "    #but then we're calling y_train[i]... well i guess that's fine. kinda like a neat workaround. iteration runs the same\n",
    "    #number/order as indexes, so that works\n",
    "    #hmmm, even when i tried to replicate this and do a for-loop w/ enumerate w/ arrays, it still acted the same as simply doing zip!!!\n",
    "    #so i wonder-- is that what we're actually going for? or, does it just LOOK like this output/display but in actuality\n",
    "    #it is tupled the way we want? i.e. in effect/practice, the elements are PAIRED-WEDDED-BONDED the way we want???\n",
    "    \n",
    "    \n",
    "    \n",
    "    #####QUESTION#####\n",
    "\n",
    "\n",
    "    \n",
    "    new_array = (j, y_train[i])\n",
    "    # Append the values to the array (empty LIST we initialized) that will be shuffled\n",
    "    X_train_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays\n",
    "X_train_shuffle = np.array(X_train_shuffle)\n",
    "\n",
    "\n",
    "#okay so we CALL IT \"shuffle\" but it's not shuffled at all? it's perfectly lined up? does that come later or something?\n",
    "#why not do it now and finish it off?\n",
    "#coulda done here:\n",
    "#np.random.shuffle(X_train_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array for the test data set\n",
    "\n",
    "X_test_shuffle = []\n",
    "\n",
    "# It is necessary to create a for loop with enumeration as well\n",
    "for i,j in enumerate(X_test):\n",
    "    # The new array would be the array containing the image plus its label \n",
    "    new_array = (j, y_test[i])\n",
    "    # Append the values to the array that will be shuffled\n",
    "    X_test_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays  \n",
    "X_test_shuffle = np.array(X_test_shuffle)\n",
    "\n",
    "#coulda done here:\n",
    "#np.random.shuffle(X_test_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the random shuffle to make the train and test with no specific order\n",
    "\n",
    "np.random.shuffle(X_train_shuffle)\n",
    "\n",
    "np.random.shuffle(X_test_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate between the X_train and y_train to fit the model\n",
    "\n",
    "#we already have these arrays tho w/ these names... so we're overwriting them??\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Start a for loop into the X_train_shuffle\n",
    "for i in X_train_shuffle:  #remem, X_train_shuffle is a set of shuffled up PAIRS of images w/ their corresponding values\n",
    "                           #so that's the great thing - we don't risk mixing up the data since we're shuffling PAIRS!!! keeping them intact\n",
    "    #so each 'i' is a tuple pair of (image, value)\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0] #ohh okay, so this is referencing the 0-index of EACH i-element-tuple-pair within X_train_shuffle,\n",
    "    #which is the IMAGE!! cuz i([[0],[1]]) = X_train_shuffle([[0],[1]]) = X_train_shuffle([[image],[value]])!!!\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1] #and so the 1-index-position of each tuple-pair-element in X_train_shuffle is the value!!!\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_train.append(value)\n",
    "    y_train.append(label)\n",
    "\n",
    "#hmmm okay, so now we store ALL the X_train images in one list(? or array?), SHUFFLED\n",
    "#oh - check below - again, not shure why didn't make arrays directly up here but yeah, these above as they stand are LISTS!\n",
    "#and do the same for the y-values\n",
    "#BUT REMEMBER! they were shuffled TOGETHER! as PAIRS! so these will STILL line up if we wanna join em back together!\n",
    "#and if they're lists we can even re-tuple/combine them now easily!\n",
    "\n",
    "\n",
    "#but again - question is - why do we wanna separate em into diff lists? i get it tho that if this is something we wanted\n",
    "#to do, we couldn't have simply taken these lists independently CUZ THE WHOLE POINT WAS TO TUPLE-PAIR-TIE THEM!!!\n",
    "    \n",
    "#but is it necessary to do a for loop to get the 'columns' of this tupled array? hmm doesn't seem so - remem how this\n",
    "#'tupled' array object looks like? seems to behave how you'd expect off that - nothing you can really do w/ it?\n",
    "\n",
    "# Divide between X_train and y_train to run model\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the test data set\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "# Start a for loop into the X_test_shuffle\n",
    "for i in X_test_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    value = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_test.append(value)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(462, 100, 70, 3)\n",
      "(462,)\n",
      "(123, 100, 70, 3)\n",
      "(123,)\n"
     ]
    }
   ],
   "source": [
    "# Make sure shapes are the same as their originals\n",
    "#bc all we did, after all that, was essentially SHUFFLE it - didn't alter / edit / add / remove any data/points!\n",
    "\n",
    "#?\n",
    "#this shows checking the shapes/lengths of each of these arrays after we JUST updated them\n",
    "#so only way to check the shape against the originals is if we made new names for these, OR have those printed out above\n",
    "#also, this says/(said) 'Make sure labels are the same as the first shapes' --> i think instead of 'labels' it means\n",
    "#shape presumably? so i changed it/wording\n",
    "\n",
    "#give new names above so we can differentiate and keep rather than overwrite and compare side by side here! put text\n",
    "#like print('The shape of X_train originally was %X_train.shape% and now it is %X_train_shuffled.shape%')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function with the neural networks\n",
    "\n",
    "def neural_network():  #what does it mean if it has no args again?\n",
    "    model = Sequential() #?\n",
    "    #whoaa - what does all this mean??\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer='he_uniform', \n",
    "                     padding = 'same', input_shape=(100, 70, 3))) #so this is the second part of the shape it gives for\n",
    "    model.add(MaxPooling2D((2, 2))) #our lists, as seen right above, that i don't understand\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#######WOWZA!!!! CAN'T WAIT TO LEARN THIS!!!##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x17fe540d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x17fe540d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 10:27:13.241837: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-01-07 10:27:13.241979: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 35ms/step - loss: 167877.3410 - accuracy: 0.4722\n",
      "Epoch 2/15\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6940 - accuracy: 0.4459\n",
      "Epoch 3/15\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.6929 - accuracy: 0.5451\n",
      "Epoch 4/15\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.6924 - accuracy: 0.5658\n",
      "Epoch 5/15\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.6921 - accuracy: 0.5601\n",
      "Epoch 6/15\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6920 - accuracy: 0.5484\n",
      "Epoch 7/15\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6924 - accuracy: 0.5293\n",
      "Epoch 8/15\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 0.6920 - accuracy: 0.5384\n",
      "Epoch 9/15\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.6913 - accuracy: 0.5506\n",
      "Epoch 10/15\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6912 - accuracy: 0.5462\n",
      "Epoch 11/15\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.6910 - accuracy: 0.5476\n",
      "Epoch 12/15\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.6922 - accuracy: 0.5235\n",
      "Epoch 13/15\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.6905 - accuracy: 0.5515\n",
      "Epoch 14/15\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 0.6899 - accuracy: 0.5570\n",
      "Epoch 15/15\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.6879 - accuracy: 0.5830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17f4d5070>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "#little confusing - so we're naming our call of the neural_network function as 'model'\n",
    "#but in the function model = Sequential()\n",
    "#and then everything is built on top of / supplemented to 'model'\n",
    "#like what would happen if we named the below something else? is it like w/ the way we wrote the function they/it has\n",
    "#to match?\n",
    "\n",
    "model = neural_network()\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x287cec1f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x287cec1f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions (predict the 'y'-value, aka the label, whether it's a flip or not flip, based on the training\n",
    "#we just did above)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get them into 0 and 1 values\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "# Start a for loop to iterate over the predictions array\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:\n",
    "        binary_values.append(1)\n",
    "\n",
    "#but how does it know what a 0 means and what a 1 means? like we assigned the 0's and 1's earlier for/as the\n",
    "#correct answers. but how does it know to call a flip a 1 and a don't flip a 0?\n",
    "#ohhh okay i get it! so we TRAINED it to identify things as either a 0 or a 1!!! cuz it can only understand numbers!\n",
    "#it has no idea what they actually are or what they mean/represent!!! thus it's PREDICTIONS will be like a weighted\n",
    "#average of how much it leans toward it being a 0/notflip or 1/flip. and we force it to pick one cuz ultimately it\n",
    "#needs to make a decision. in the final analysis of accuracy since there's diff metrics i'm sure there's on that looks\n",
    "#at the actual value to see how close it was. kinda like pass/fail but there of course still has to be an actual score!\n",
    "#it's just that you have to draw a line/make a cutoff somewhere\n",
    "        \n",
    "#hmm okay, so it's training/fitting the model using X_train & y_train, and then from that we make predictions\n",
    "#of the values of X_test to see how they compare to y_test! and so remem the predicted values will be WEIGHTED in their\n",
    "#raw form, so like if we're classifying w/ 2 things/choices, it can only be one or the other, but it will spit back\n",
    "#like *HOW MUCH IT THINKS/leans toward it being one or the other*, and so we have to tell it at what point/percentage/\n",
    "#confidence we want it to make a call for one or the other, what's the defining/differentiating/determining/\n",
    "#identifying/distinguishing/DECISION line?!!!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the confusion matrix to evaluate the model\n",
    "\n",
    "#what's that!??\n",
    "#ohhhh okay, i get it now. look at the axes. so you're seeing how well the common labels lined up. i.e. where an image\n",
    "#was both PREDICTED to be a flip AND was ACTUALLY a flip, as determined/stated by the label we gave it!\n",
    "#so in time of writing this, it's showing, of the 102 test images, all 72 were categorized as flips were indeed labeled\n",
    "#as flips. but actually,... EVERYTHING was labeled as a flip!!! so all 50 NOT FLIPS were ALSO labeled as flips!! it\n",
    "#didn't label ANYTHING as a not flip!!! lol\n",
    "\n",
    "cm = confusion_matrix(y_test, binary_values)\n",
    "#so the y_test is now giving the ANSWER KEY so the algorithm can see how it did / its performance on this test/section\n",
    "#so we gave the algo/machine this test set of images, combined flip and not flip so it didn't know which was which,\n",
    "#all it had was the model it was trained on w/ the training set of images and corresponding labels. that was the\n",
    "#instruction/guided section. this was its turn to do it on its own/try its hand at it\n",
    "#so you see that it's actually very simple here, it's comparing the machine's answers, in 'binary_values', defined/\n",
    "#assigned above, and comparing them to the CORRECT ANSWERS, as defined by us in y_test!!! confusion matrix is simply\n",
    "#just a visual tool to help us see the performance w/ machine learning prediction performance\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['dont_flip','flip'])\n",
    "cmd.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        51\n",
      "           1       0.59      1.00      0.74        72\n",
      "\n",
      "    accuracy                           0.59       123\n",
      "   macro avg       0.29      0.50      0.37       123\n",
      "weighted avg       0.34      0.59      0.43       123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the Classification report to get the precision, recall, f1-score\n",
    "\n",
    "#whatre thoooooooose!\n",
    "\n",
    "print(classification_report(y_test, binary_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### I was able to create a model with 0.99 accuracy for whether a page needs to be flipped or not by using deep learning and doing the necessary data preprocessing such as making all the pages the same size, adjusting brightness, adding noise, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x294a95040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x294a95040> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 10:27:21.489569: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: flip_page_classifier/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model using pickle\n",
    "\n",
    "#doesn't say pickle anywhere??? we did import it at start/top tho\n",
    "\n",
    "model_classifier = model.save('flip_page_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wait so what's the point of making this video?? what's different about the info we learn from a video format vs.\n",
    "#picture?? they wouldn't be the same??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "MoviePy error: the file training_flip.avi could not be found!\nPlease check that you entered the correct path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cr/9gwdd0wn43d9d1db4xfhyvpc0000gn/T/ipykernel_4730/391609661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Establish the train video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvideo_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_flip.avi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvideo_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_notflip.avi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_m1/lib/python3.8/site-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Make a reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpix_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rgba\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_mask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb24\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         self.reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt,\n\u001b[0m\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_m1/lib/python3.8/site-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0m\u001b[1;32m     33\u001b[0m                                    fps_source)\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tf_m1/lib/python3.8/site-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"No such file or directory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[0m\u001b[1;32m    275\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                       \"path.\")%filename)\n",
      "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file training_flip.avi could not be found!\nPlease check that you entered the correct path."
     ]
    }
   ],
   "source": [
    "# Establish the training video by concatenating the flips with the not flips for train and test data sets.\n",
    "\n",
    "#Establish the train video\n",
    "\n",
    "video_1 = VideoFileClip('training_flip.avi')\n",
    "video_2 = VideoFileClip('training_notflip.avi')\n",
    "\n",
    "training_video = concatenate_videoclips([video_1, video_2])\n",
    "\n",
    "training_video.write_videofile('training_video.avi', codec = 'rawvideo')\n",
    "training_video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish the test video\n",
    "\n",
    "video_3 = VideoFileClip('test_flip.avi')\n",
    "video_4 = VideoFileClip('test_notflip.avi')\n",
    "\n",
    "test_video= concatenate_videoclips([video_3, video_4])\n",
    "\n",
    "test_video.write_videofile('test_video.avi', codec = 'rawvideo')\n",
    "test_video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the frames for the training video\n",
    "\n",
    "count = 0\n",
    "\n",
    "videoFile = 'training_video.avi'\n",
    "# Capturing the video from the given path\n",
    "cap = cv2.VideoCapture(videoFile)   \n",
    "# Establish Frame rate\n",
    "frameRate = cap.get(5) \n",
    "\n",
    "x=1\n",
    "\n",
    "filenames_train = []\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True): \n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"frame%d.jpg\" % count;count+=1\n",
    "        filenames_train.append(filename)\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the frames from the test video\n",
    "\n",
    "count = 0\n",
    "\n",
    "videoFile = 'test_video.avi'\n",
    "# Capturing the video from the given path\n",
    "cap = cv2.VideoCapture(videoFile)   \n",
    "# Establish Frame rate\n",
    "frameRate = cap.get(5) \n",
    "\n",
    "x=1\n",
    "\n",
    "filenames_test = []\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"frame%d.jpg\" % count;count+=1\n",
    "        filenames_test.append(filename)\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the y_train flip and not flip into the same array\n",
    "\n",
    "y_train = np.append(np.array(y_train_flip), np.array(y_train_notflip))\n",
    "\n",
    "# Append the y_test flip and not flip into the same array\n",
    "\n",
    "y_test = np.append(np.array(y_test_flip), np.array(y_test_notflip))\n",
    "\n",
    "# Create the data frame that will show the frameID and the class for the train data\n",
    "\n",
    "data_train = pd.DataFrame({'frameID': filenames_train, 'flip': y_train})\n",
    "\n",
    "# Create the data frame that will show the frameID and the class for the test data\n",
    "\n",
    "data_test = pd.DataFrame({'frameID': filenames_test, 'flip': y_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly change the data order and reset the index\n",
    "\n",
    "data_train = shuffle(data_train).reset_index(drop = True)\n",
    "\n",
    "data_test = shuffle(data_test).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array\n",
    "\n",
    "X_train = []   \n",
    "\n",
    "# Loop through the frameID column and store every frame in X\n",
    "for img_name in data_train.frameID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    X_train.append(img)  \n",
    "    \n",
    "# Convert the list to an array\n",
    "X_train = np.array(X_train)    \n",
    "\n",
    "# Define the y_train\n",
    "\n",
    "y_train = data_train['flip'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array\n",
    "\n",
    "X_test = []   \n",
    "\n",
    "# Loop through the frameID column and store every frame in X\n",
    "for img_name in data_test.frameID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    X_test.append(img)  \n",
    "    \n",
    "# Convert the list to an array\n",
    "\n",
    "X_test = np.array(X_test) \n",
    "\n",
    "# Define the y_test\n",
    "\n",
    "y_test = data_test['flip'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to do the preprocessing for each frame of image of the video\n",
    "\n",
    "def image_preprocessing_frame(data):\n",
    "    # Create an empty list to store all the preprocessed images\n",
    "    images = []\n",
    "    # Start by creating a for loop through all the path and make the preprocessing to each image\n",
    "    for i in data:\n",
    "        # Adjust the size so all iamges will have the same size\n",
    "        img = cv2.resize(i, dsize = (70,140), interpolation=cv2.INTER_CUBIC)\n",
    "        # Crop to remove part of the images I don't need for the modeling part\n",
    "        y,h,x,w = 0,100,0,70\n",
    "        img = img[y:y+h, x:x+w]\n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5\n",
    "        beta=0.5\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "        # Create the video\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the defined function to preprocess the data\n",
    "\n",
    "X_train = image_preprocessing_frame(data = X_train)\n",
    "\n",
    "X_test = image_preprocessing_frame(data = X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include_top=False to remove the top layer and a base model\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 70, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list into arrays\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make predictions using this model for X_train and X_valid, get the features, and then use those \n",
    "# features to retrain the model.\n",
    "\n",
    "X_train = base_model.predict(X_train)\n",
    "\n",
    "X_test = base_model.predict(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centering the data\n",
    "\n",
    "X_train = X_train/X_train.max()\n",
    "\n",
    "X_test = X_test/X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network to use to predict if the frame of the video is flip or not flip\n",
    "\n",
    "def model_neural():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation = 'relu', kernel_initializer='he_uniform', \n",
    "                     padding = 'same', input_shape=(3, 2, 512)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function with the created model\n",
    "\n",
    "model = model_neural()\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Get them into 0 and 1 values\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "# Start a for loop to iterate over the predictions array\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:\n",
    "        binary_values.append(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix to evaluate the model\n",
    "\n",
    "cm = confusion_matrix(y_test, binary_values)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['not_flip','flip'])\n",
    "cmd.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Classification report to get the precision, recall, f1-score\n",
    "\n",
    "print(classification_report(y_test, binary_values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
