{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Activation, MaxPooling2D, Conv2D, InputLayer)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "from numba import double, jit, njit, vectorize\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, ConfusionMatrixDisplay)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_training_flip   = glob.glob('Houses Dataset/Training Set/Flip_Mixed_Training/Flip_Kitchen_Training/*.jpg')\n",
    "path_training_noflip = glob.glob('Houses Dataset/Training Set/Dont Flip_Mixed_Training/Dont Flip_Kitchen_Training/*.jpg')\n",
    "\n",
    "path_testing_flip   = glob.glob('Houses Dataset/Testing Set/Flip_Mixed_Testing/Flip_Kitchen_Testing/*.jpg')\n",
    "path_testing_noflip = glob.glob('Houses Dataset/Testing Set/Dont Flip_Mixed_Testing/Dont Flip_Kitchen_Testing/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(path):\n",
    "    \n",
    "    images = []\n",
    "\n",
    "    for i in path:\n",
    "        img = cv2.imread(i)\n",
    "        img = cv2.resize(img, dsize = (32,32), interpolation=cv2.INTER_CUBIC)\n",
    "        y, h, x, w = 2, 30, 2, 30 #evenly cut outside border around all sides\n",
    "        img = img[y:h, x:w]\n",
    "\n",
    "        alpha=1.5\n",
    "        beta=0.5\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        img = cv2.bitwise_not(img)\n",
    "        img = img/255               \n",
    "\n",
    "        images.append(img)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_training_flip = image_preprocessing(path_training_flip)\n",
    "\n",
    "img_training_noflip = image_preprocessing(path_training_noflip)\n",
    "\n",
    "img_testing_flip = image_preprocessing(path_testing_flip)\n",
    "\n",
    "img_testing_noflip = image_preprocessing(path_testing_noflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 69, 17, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_training_flip),len(img_training_noflip),len(img_testing_flip),len(img_testing_noflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_flip =   [1 for i in range(0, len(img_training_flip))]\n",
    "\n",
    "y_train_noflip = [0 for i in range(0, len(img_training_noflip))]\n",
    "\n",
    "y_test_flip =    [1 for i in range(0, len(img_testing_flip))]\n",
    "\n",
    "y_test_noflip =  [0 for i in range(0, len(img_testing_noflip))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((img_training_flip, img_training_noflip))\n",
    "\n",
    "X_test =  np.concatenate((img_testing_flip, img_testing_noflip))\n",
    "\n",
    "y_train = np.append(y_train_flip, y_train_noflip)\n",
    "\n",
    "y_test =  np.append(y_test_flip, y_test_noflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 28, 28, 3)\n",
      "(138,)\n",
      "(34, 28, 28, 3)\n",
      "(34,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train = np.array(list(zip(X_train,y_train)))\n",
    "\n",
    "np.random.shuffle(Xy_train)\n",
    "\n",
    "Xy_train_shuffle = Xy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test = np.array(list(zip(X_test,y_test)))\n",
    "\n",
    "np.random.shuffle(Xy_test)\n",
    "\n",
    "Xy_test_shuffle = Xy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shuffled = []\n",
    "y_train_shuffled = []\n",
    "\n",
    "for i in Xy_train_shuffle:\n",
    "    image = i[0]\n",
    "    X_train_shuffled.append(image)\n",
    "    label = i[1]\n",
    "    y_train_shuffled.append(label)\n",
    "\n",
    "X_train_shuffled = np.array(X_train_shuffled)\n",
    "\n",
    "y_train_shuffled = np.array(y_train_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_shuffled = []\n",
    "y_test_shuffled = []\n",
    "\n",
    "for i in Xy_test_shuffle:\n",
    "    image = i[0]\n",
    "    X_test_shuffled.append(image)\n",
    "    label = i[1]\n",
    "    y_test_shuffled.append(label)\n",
    "\n",
    "X_test_shuffled = np.array(X_test_shuffled)\n",
    "\n",
    "y_test_shuffled = np.array(y_test_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 28, 28, 3)\n",
      "(138,)\n",
      "(34, 28, 28, 3)\n",
      "(34,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_shuffled.shape)\n",
    "print(y_train_shuffled.shape)\n",
    "print(X_test_shuffled.shape)\n",
    "print(y_test_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu', kernel_initializer='he_uniform', input_shape=(28, 28, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt = SGD(lr=.001, momentum=.9)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics='accuracy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x28b952ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x28b952ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7248 - accuracy: 0.5139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 18:49:00.416544: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-01-17 18:49:00.416714: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6656 - accuracy: 0.5962\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6043 - accuracy: 0.7169\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5500 - accuracy: 0.7362\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5128 - accuracy: 0.9014\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4851 - accuracy: 0.7655\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4712 - accuracy: 0.8504\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4334 - accuracy: 0.8922\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.4263 - accuracy: 0.8530\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4174 - accuracy: 0.8928\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3605 - accuracy: 0.9003\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3592 - accuracy: 0.9110\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3271 - accuracy: 0.9288\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3084 - accuracy: 0.9468\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2873 - accuracy: 0.9462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28b9a90d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = neural_network()\n",
    "\n",
    "model.fit(X_train_shuffled, y_train_shuffled, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x28c003d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x28c003d30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_shuffled)\n",
    "\n",
    "binary_values = []\n",
    "\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    else:\n",
    "        binary_values.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmx = confusion_matrix(y_test_shuffled, binary_values)\n",
    "\n",
    "cmd = ConfusionMatrixDisplay(cmx, display_labels=['dont flip','flip'])\n",
    "cmd.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.76      0.81        17\n",
      "           1       0.79      0.88      0.83        17\n",
      "\n",
      "    accuracy                           0.82        34\n",
      "   macro avg       0.83      0.82      0.82        34\n",
      "weighted avg       0.83      0.82      0.82        34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_shuffled, binary_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x293880940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x293880940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: flip_page_classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 18:49:01.307736: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "model_classifier = model.save('flip_house_classifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c476d5be062af8c53c5a2e4f951b2f07aa9c47f05899e43528a06f0dd12534bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
