{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Estate Investment Opportunity Identification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting whether or not a real estate property is a good candidate for flipping (whether to buy and sell or buy and rent, but in either case, to RENOVATE, for high returns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline\n",
    "\n",
    "1. Data Wrangling\n",
    "2. Pre-Processing\n",
    "3. Modeling\n",
    "4. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2 #to read images\n",
    "import glob #to tell it what kind of files to read within the filepath, in this case .jpg's\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image   # for preprocessing the images\n",
    "from tensorflow.keras.utils import to_categorical #np_utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Activation, MaxPooling2D, Conv2D, InputLayer)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "from numba import double, jit, njit, vectorize\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, ConfusionMatrixDisplay)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pickle\n",
    "\n",
    "from skimage.transform import resize   #for resizing images\n",
    "\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tool we are making is designed work from images ALONE, the wrangling phase is to get the images into the right format.\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image paths\n",
    "\n",
    "#we'll use an ~80/20 split for training/testing\n",
    "\n",
    "#Training\n",
    "path_training_flip   = glob.glob('Houses Dataset/Training Set/Flip_Mixed_Training/Flip_Kitchen_Training/*.jpg')\n",
    "path_training_noflip = glob.glob('Houses Dataset/Training Set/Dont Flip_Mixed_Training/Dont Flip_Kitchen_Training/*.jpg')\n",
    "\n",
    "#Testing\n",
    "path_testing_flip   = glob.glob('Houses Dataset/Testing Set/Flip_Mixed_Testing/Flip_Kitchen_Testing/*.jpg')\n",
    "path_testing_noflip = glob.glob('Houses Dataset/Testing Set/Dont Flip_Mixed_Testing/Dont Flip_Kitchen_Testing/*.jpg')\n",
    "\n",
    "#glob looks for all files/filepaths that follow/contain a specified pattern, using the *wild card, so here *.jpg!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Procedure:\n",
    "\n",
    "    -Generate the training & test images sets, each having a separate one for sell/not sell\n",
    "    -Make list of labels for the images\n",
    "    -Combine the training images; Combine the testing images; And same for labels\n",
    "    -Combine the training images with their labels; Same for testing\n",
    "    -Shuffle each\n",
    "    -Separate the labels from the images in each\n",
    "\n",
    "That will get us Xtest, Xtrain, ytest, ytrain thus readying us for modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess images\n",
    "\n",
    "def image_preprocessing(path): #works using the filepaths just made\n",
    "    \n",
    "    #empty list to store all finished, preprocessed images\n",
    "    images = []\n",
    "\n",
    "    #loop through all the path images, preprocessing each one\n",
    "\n",
    "    for i in path:\n",
    "        #read the image\n",
    "        img = cv2.imread(i)\n",
    "        # Adjust the size so all images will be the same size\n",
    "        img = cv2.resize(img, dsize = (32,32), interpolation=cv2.INTER_CUBIC)\n",
    "        #what's right size??\n",
    "        ########################################################################################\n",
    "        #interpolation tells it how to \"create new pixels\" to make the image look smoother as you make it bigger, aka *FILL-IN* the gaps!!!\n",
    "        #rather than just stretch the original pixels and risking \"pixely\" images\n",
    "        #dsize (desired size?) is pixel height/width\n",
    "        #bicubic is the smoothest cuz it's curved/polynomial\n",
    "\n",
    "        # Crop to remove excess/unnecessary part of images, like around the border/near the edges, noise, watermarks, text etc\n",
    "        # this way we can stay focused\n",
    "        \n",
    "        #starting/ending pixels\n",
    "        img = img[4:26, 4:28]\n",
    "        #top is usually just ceiling of room which isn't very useful, unnecessary noise\n",
    "        \n",
    "\n",
    "        # Adjust brightness, contrast\n",
    "        alpha=1.5 #contrast/gain...wait, but then somewhere else it says *WEIGHT* of first image??\n",
    "        beta=0.5  #brightness/bias..and this says elswhere weight of SECOND image!?!\n",
    "        #gamma=??\n",
    "        img = cv2.addWeighted(img, alpha, np.zeros(img.shape, img.dtype), 0, beta)\n",
    "        ########################################################################################\n",
    "        #addWeighted helps to blend/transition the two images together, by specifiying respective weights / alpha, beta and gamma values, like how visible\n",
    "        #or transparent one is\n",
    "        \n",
    "    \n",
    "        #so this bitwise_not step along is inversing it, does that mean it alone is making the image grayscale?! or is it only in/w combination w/ the\n",
    "        #division by 255??\n",
    "        ########################################################################################\n",
    "        img = cv2.bitwise_not(img)\n",
    "        img = img/255               \n",
    "\n",
    "\n",
    "        #it would seem that dividing by 255 *ISN'T* converting colorscheme to grayscale but rather simply simplifying the RGB numbers of each pixel to something\n",
    "        #easier to interpret?? b/w 0/1!\n",
    "        ########################################################################################\n",
    "\n",
    "        # Append the img to the list images\n",
    "        images.append(img)\n",
    "\n",
    "    # Return the list with the preprocessed images\n",
    "    return images\n",
    "\n",
    "#okay so overall, this is iterating thru our images in our path folder and reshaping/resizing/recoloring them and\n",
    "#tryna crop out the background noise as much as possible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the training data\n",
    "\n",
    "#these are the X's\n",
    "\n",
    "img_training_flip = image_preprocessing(path_training_flip)\n",
    "\n",
    "img_training_noflip = image_preprocessing(path_training_noflip)\n",
    "\n",
    "img_testing_flip = image_preprocessing(path_testing_flip)\n",
    "\n",
    "img_testing_noflip = image_preprocessing(path_testing_noflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_training_flip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cr/9gwdd0wn43d9d1db4xfhyvpc0000gn/T/ipykernel_53688/3269502332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#take a look at the pixel RGB expression of the images!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimg_training_flip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'img_training_flip' is not defined"
     ]
    }
   ],
   "source": [
    "#take a look at the pixel RGB expression of the images!\n",
    "img_training_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 69, 17, 17)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check you extracted the number of images you were intending in each group. we're trying to have the exact same number for the 2 categories\n",
    "#to avoid unnecessary class imbalance\n",
    "#if we see the numbers don't match, that could mean we forgot to change one of the file formats to .jpg (could be .png or .webpage)\n",
    "len(img_training_flip),len(img_training_noflip),len(img_testing_flip),len(img_testing_noflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the labels\n",
    "\n",
    "y_train_flip =   [1 for i in range(0, len(img_training_flip))]\n",
    "\n",
    "y_train_noflip = [0 for i in range(0, len(img_training_noflip))]\n",
    "\n",
    "y_test_flip =    [1 for i in range(0, len(img_testing_flip))]\n",
    "\n",
    "y_test_noflip =  [0 for i in range(0, len(img_testing_noflip))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the separate categories into X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train = np.concatenate((img_training_flip, img_training_noflip))\n",
    "\n",
    "X_test =  np.concatenate((img_testing_flip, img_testing_noflip))\n",
    "\n",
    "y_train = np.append(y_train_flip, y_train_noflip)\n",
    "\n",
    "y_test =  np.append(y_test_flip, y_test_noflip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.7254902 , 0.56862745, 0.34901961],\n",
       "         [0.61568627, 0.45490196, 0.25490196],\n",
       "         [0.74509804, 0.54509804, 0.29019608],\n",
       "         ...,\n",
       "         [0.76862745, 0.63137255, 0.4745098 ],\n",
       "         [0.80392157, 0.6745098 , 0.4627451 ],\n",
       "         [0.40392157, 0.25098039, 0.03921569]],\n",
       "\n",
       "        [[0.59607843, 0.43137255, 0.23137255],\n",
       "         [0.82745098, 0.67843137, 0.4745098 ],\n",
       "         [0.66666667, 0.53333333, 0.34509804],\n",
       "         ...,\n",
       "         [0.61960784, 0.41568627, 0.18039216],\n",
       "         [0.78431373, 0.63137255, 0.35686275],\n",
       "         [0.34117647, 0.20784314, 0.        ]],\n",
       "\n",
       "        [[0.44313725, 0.2745098 , 0.09019608],\n",
       "         [0.6745098 , 0.49019608, 0.24313725],\n",
       "         [0.81568627, 0.70196078, 0.45098039],\n",
       "         ...,\n",
       "         [0.50196078, 0.33333333, 0.10196078],\n",
       "         [0.71372549, 0.58039216, 0.28627451],\n",
       "         [0.64705882, 0.4745098 , 0.20784314]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.8627451 , 0.71372549, 0.49803922],\n",
       "         [0.83137255, 0.70980392, 0.48627451],\n",
       "         [0.80392157, 0.71372549, 0.56078431],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.00784314],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.80392157, 0.6745098 , 0.4627451 ],\n",
       "         [0.78039216, 0.65098039, 0.43921569],\n",
       "         [0.85098039, 0.78431373, 0.62745098],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.00784314],\n",
       "         [0.        , 0.        , 0.01176471],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.80392157, 0.66666667, 0.45490196],\n",
       "         [0.78431373, 0.64313725, 0.43137255],\n",
       "         [0.65490196, 0.49019608, 0.38431373],\n",
       "         ...,\n",
       "         [0.        , 0.00784314, 0.03921569],\n",
       "         [0.        , 0.        , 0.02745098],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.66666667, 0.26666667, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.56862745, 0.61960784, 0.60392157],\n",
       "         [0.63921569, 0.65490196, 0.61568627],\n",
       "         [0.4745098 , 0.46666667, 0.43529412]],\n",
       "\n",
       "        [[0.6627451 , 0.25098039, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.38431373, 0.38431373, 0.35686275],\n",
       "         [0.38039216, 0.36862745, 0.34117647],\n",
       "         [0.44313725, 0.42745098, 0.39607843]],\n",
       "\n",
       "        [[0.66666667, 0.25098039, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.31372549, 0.30980392, 0.2627451 ],\n",
       "         [0.39215686, 0.37254902, 0.32156863],\n",
       "         [0.76470588, 0.67843137, 0.58039216]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.81960784, 0.43921569, 0.07843137],\n",
       "         [0.85490196, 0.53333333, 0.14901961],\n",
       "         [0.0627451 , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.11372549, 0.16862745, 0.10196078],\n",
       "         [0.21960784, 0.22745098, 0.15294118],\n",
       "         [0.45098039, 0.45098039, 0.38823529]],\n",
       "\n",
       "        [[0.83137255, 0.4627451 , 0.10980392],\n",
       "         [0.85490196, 0.53333333, 0.15686275],\n",
       "         [0.07843137, 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.65098039, 0.68627451, 0.6627451 ],\n",
       "         [0.78039216, 0.82745098, 0.80392157],\n",
       "         [0.18431373, 0.25098039, 0.21568627]],\n",
       "\n",
       "        [[0.85490196, 0.50196078, 0.1372549 ],\n",
       "         [0.81568627, 0.50196078, 0.1372549 ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.90980392, 0.94901961, 0.97254902],\n",
       "         [0.82745098, 0.8627451 , 0.85882353],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.12156863, 0.01960784, 0.        ],\n",
       "         [0.11372549, 0.00392157, 0.        ],\n",
       "         [0.1372549 , 0.01960784, 0.        ],\n",
       "         ...,\n",
       "         [0.58039216, 0.49019608, 0.41960784],\n",
       "         [0.41568627, 0.34509804, 0.2627451 ],\n",
       "         [0.34117647, 0.22352941, 0.12941176]],\n",
       "\n",
       "        [[0.13333333, 0.04313725, 0.        ],\n",
       "         [0.1254902 , 0.03137255, 0.        ],\n",
       "         [0.10980392, 0.00392157, 0.        ],\n",
       "         ...,\n",
       "         [0.43137255, 0.2627451 , 0.11372549],\n",
       "         [0.27843137, 0.19215686, 0.10588235],\n",
       "         [0.2745098 , 0.11372549, 0.        ]],\n",
       "\n",
       "        [[0.09019608, 0.04313725, 0.        ],\n",
       "         [0.12156863, 0.05098039, 0.        ],\n",
       "         [0.09803922, 0.00784314, 0.        ],\n",
       "         ...,\n",
       "         [0.40392157, 0.20784314, 0.09019608],\n",
       "         [0.30980392, 0.19607843, 0.11372549],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6627451 , 0.65098039, 0.64313725],\n",
       "         [0.65490196, 0.63137255, 0.62745098],\n",
       "         [0.67843137, 0.65098039, 0.64313725],\n",
       "         ...,\n",
       "         [0.21960784, 0.1254902 , 0.05098039],\n",
       "         [0.35686275, 0.22745098, 0.12156863],\n",
       "         [0.34509804, 0.25490196, 0.15686275]],\n",
       "\n",
       "        [[0.6627451 , 0.64313725, 0.63921569],\n",
       "         [0.67843137, 0.65490196, 0.65098039],\n",
       "         [0.65098039, 0.62745098, 0.61568627],\n",
       "         ...,\n",
       "         [0.12156863, 0.05098039, 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.34117647, 0.24705882, 0.19215686]],\n",
       "\n",
       "        [[0.65098039, 0.62745098, 0.61960784],\n",
       "         [0.78039216, 0.76078431, 0.75686275],\n",
       "         [0.58039216, 0.54901961, 0.5372549 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.03921569, 0.        , 0.        ],\n",
       "         [0.02745098, 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.95686275, 0.78039216, 0.61960784],\n",
       "         [0.68627451, 0.4627451 , 0.26666667],\n",
       "         [0.68627451, 0.39607843, 0.15686275],\n",
       "         ...,\n",
       "         [0.96862745, 0.94509804, 0.75686275],\n",
       "         [0.99607843, 0.96078431, 0.75686275],\n",
       "         [0.99215686, 0.92156863, 0.74509804]],\n",
       "\n",
       "        [[0.89019608, 0.6745098 , 0.51372549],\n",
       "         [0.73333333, 0.58431373, 0.45098039],\n",
       "         [0.59215686, 0.34509804, 0.13333333],\n",
       "         ...,\n",
       "         [0.99215686, 0.92156863, 0.69803922],\n",
       "         [0.99215686, 0.92156863, 0.70980392],\n",
       "         [1.        , 0.92941176, 0.75686275]],\n",
       "\n",
       "        [[0.72156863, 0.4627451 , 0.26666667],\n",
       "         [0.61960784, 0.40392157, 0.20784314],\n",
       "         [0.65490196, 0.39607843, 0.18039216],\n",
       "         ...,\n",
       "         [0.80392157, 0.36078431, 0.        ],\n",
       "         [0.94509804, 0.89803922, 0.69803922],\n",
       "         [0.96078431, 0.89803922, 0.71764706]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.99215686, 0.88627451, 0.69019608],\n",
       "         [0.99607843, 0.99607843, 0.84313725],\n",
       "         [0.99607843, 0.94509804, 0.80784314],\n",
       "         ...,\n",
       "         [0.61568627, 0.20784314, 0.        ],\n",
       "         [0.45490196, 0.08627451, 0.        ],\n",
       "         [0.6627451 , 0.29803922, 0.        ]],\n",
       "\n",
       "        [[0.98431373, 0.89019608, 0.70196078],\n",
       "         [0.87843137, 0.81960784, 0.66666667],\n",
       "         [0.85098039, 0.59607843, 0.42745098],\n",
       "         ...,\n",
       "         [0.55686275, 0.39607843, 0.07843137],\n",
       "         [0.76078431, 0.65098039, 0.49019608],\n",
       "         [0.73333333, 0.30196078, 0.        ]],\n",
       "\n",
       "        [[0.71372549, 0.42745098, 0.22745098],\n",
       "         [0.66666667, 0.33333333, 0.10196078],\n",
       "         [0.58039216, 0.19607843, 0.        ],\n",
       "         ...,\n",
       "         [0.97254902, 0.82745098, 0.61960784],\n",
       "         [0.94901961, 0.96078431, 0.9372549 ],\n",
       "         [0.70980392, 0.58431373, 0.41960784]]],\n",
       "\n",
       "\n",
       "       [[[0.7254902 , 0.41960784, 0.13333333],\n",
       "         [0.70980392, 0.39215686, 0.09803922],\n",
       "         [0.67843137, 0.36078431, 0.06666667],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.73333333, 0.43137255, 0.14509804],\n",
       "         [0.70980392, 0.41568627, 0.1254902 ],\n",
       "         [0.69019608, 0.39607843, 0.10980392],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.71372549, 0.45490196, 0.17254902],\n",
       "         [0.70980392, 0.43137255, 0.15686275],\n",
       "         [0.69019608, 0.40392157, 0.1254902 ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.01176471, 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.80392157, 0.7372549 , 0.50196078],\n",
       "         [0.94509804, 0.88627451, 0.59607843],\n",
       "         [0.69019608, 0.52156863, 0.12156863],\n",
       "         ...,\n",
       "         [0.84313725, 0.78039216, 0.38039216],\n",
       "         [0.42745098, 0.25490196, 0.        ],\n",
       "         [0.45098039, 0.31764706, 0.        ]],\n",
       "\n",
       "        [[0.85098039, 0.78039216, 0.54509804],\n",
       "         [0.94509804, 0.89019608, 0.62745098],\n",
       "         [0.79215686, 0.69803922, 0.32156863],\n",
       "         ...,\n",
       "         [0.82745098, 0.65098039, 0.18039216],\n",
       "         [0.60392157, 0.42745098, 0.05882353],\n",
       "         [0.52941176, 0.39215686, 0.02745098]],\n",
       "\n",
       "        [[0.9372549 , 0.89019608, 0.6627451 ],\n",
       "         [0.89803922, 0.8627451 , 0.60784314],\n",
       "         [0.81960784, 0.71372549, 0.39607843],\n",
       "         ...,\n",
       "         [0.81960784, 0.60784314, 0.13333333],\n",
       "         [0.64313725, 0.45098039, 0.01960784],\n",
       "         [0.60392157, 0.42745098, 0.04313725]]],\n",
       "\n",
       "\n",
       "       [[[0.7254902 , 0.61568627, 0.41960784],\n",
       "         [0.49019608, 0.38431373, 0.15686275],\n",
       "         [0.7372549 , 0.65098039, 0.4627451 ],\n",
       "         ...,\n",
       "         [0.3254902 , 0.14509804, 0.        ],\n",
       "         [0.49019608, 0.30196078, 0.09803922],\n",
       "         [0.50980392, 0.32156863, 0.11372549]],\n",
       "\n",
       "        [[0.70196078, 0.61960784, 0.40784314],\n",
       "         [0.64313725, 0.54901961, 0.33333333],\n",
       "         [0.71372549, 0.60392157, 0.40392157],\n",
       "         ...,\n",
       "         [0.3254902 , 0.13333333, 0.        ],\n",
       "         [0.56078431, 0.37254902, 0.16862745],\n",
       "         [0.58431373, 0.40392157, 0.20784314]],\n",
       "\n",
       "        [[0.48627451, 0.36862745, 0.1254902 ],\n",
       "         [0.72156863, 0.54509804, 0.23921569],\n",
       "         [0.56078431, 0.40392157, 0.06666667],\n",
       "         ...,\n",
       "         [0.53333333, 0.34901961, 0.12156863],\n",
       "         [0.58431373, 0.39607843, 0.18431373],\n",
       "         [0.65490196, 0.49019608, 0.28627451]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.82745098, 0.77254902, 0.57254902],\n",
       "         [0.84313725, 0.81568627, 0.75686275],\n",
       "         [0.76078431, 0.70196078, 0.48627451],\n",
       "         ...,\n",
       "         [0.69019608, 0.69019608, 0.59607843],\n",
       "         [0.7254902 , 0.7254902 , 0.63137255],\n",
       "         [0.57647059, 0.55294118, 0.45882353]],\n",
       "\n",
       "        [[0.89019608, 0.86666667, 0.69803922],\n",
       "         [0.89803922, 0.82745098, 0.66666667],\n",
       "         [0.8745098 , 0.81568627, 0.61960784],\n",
       "         ...,\n",
       "         [0.43137255, 0.40784314, 0.34509804],\n",
       "         [0.67843137, 0.6745098 , 0.59215686],\n",
       "         [0.72156863, 0.70980392, 0.64705882]],\n",
       "\n",
       "        [[0.99607843, 0.99607843, 0.92156863],\n",
       "         [0.76078431, 0.72156863, 0.63137255],\n",
       "         [0.66666667, 0.65098039, 0.59215686],\n",
       "         ...,\n",
       "         [0.57254902, 0.5254902 , 0.45098039],\n",
       "         [0.7254902 , 0.72156863, 0.63921569],\n",
       "         [0.71764706, 0.70980392, 0.65098039]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cr/9gwdd0wn43d9d1db4xfhyvpc0000gn/T/ipykernel_53688/4075987148.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "#see how \n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#btw - THIS IS GOOD TO SEE THAT THERE'S COMPLETE SEGREGATION AT THIS POINT BECAUSE THIS SHOWS THE APPENDED*\n",
    "#/*COMBINED* FLIP/NOT FLIP SET THAT MAKES UP/COMPRISES Y_TEST!!!!!\n",
    "#later we're gonna SHUFFLE IT UP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commented out to avoid stoppage / breakage when running whole notebook!\n",
    "# img_training_flip.shape\n",
    "# >> output:\n",
    "# ---------------------------------------------------------------------------\n",
    "# AttributeError                            Traceback (most recent call last)\n",
    "# /var/folders/cr/9gwdd0wn43d9d1db4xfhyvpc0000gn/T/ipykernel_90203/1074584411.py in <module>\n",
    "# ----> 1 img_training_flip.shape\n",
    "\n",
    "# AttributeError: 'list' object has no attribute 'shape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hain? why is this being treated like a list when it looks exactly like X_train??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 28, 28, 3)\n",
      "(138,)\n",
      "(34, 28, 28, 3)\n",
      "(34,)\n"
     ]
    }
   ],
   "source": [
    "# Check if shapes make sense and match requirements\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine the images & labels TOGETHER! so that we can SHUFFLE THEM while still keeping the correct labels INTACT! unseparated!\n",
    "# we'll have to do this SEPARATELY of course for both training & testing, cuz of leakage/peekage! (\"pack\"age;P)\n",
    "\n",
    "#training\n",
    "\n",
    "Xy_train_shuffle = []\n",
    "\n",
    "#enumerate() will create a list of tuple pairs: each image +/w its index position!\n",
    "    #so i=index of that image, j=image at that index! great couple/tuple pair!\n",
    "for i,j in enumerate(X_train):\n",
    "    # This will pair the training image itself with its truth LABEL (y)\n",
    "    new_array = (j, y_train[i])\n",
    "    #populate the list with these labeled images, which we will next shuffle!\n",
    "    Xy_train_shuffle.append(new_array)\n",
    "    \n",
    "# Have the new set of arrays\n",
    "Xy_train_shuffle = np.array(Xy_train_shuffle)\n",
    "#hmmmmm - why do we need to make it an np.array()? so this is like an array of arrays? only works in this format?\n",
    "    \n",
    "\n",
    "#shuffle\n",
    "np.random.shuffle(Xy_train_shuffle)\n",
    "\n",
    "\n",
    "#NOTE!!!!! YOU DON'T HAVE TO DO ANYTHING TO *SAVE* THE SHUFFLING ORDER!!!\n",
    "#in fact, if you save this to a variable, i think when you display that variable, it'll just EXECUTE this code!!!\n",
    "#like it's just a *set of instructions/COMMAND!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note, we call it 'X'_test_shuffle & X_train_shuffle, but it's got X+AND+y now...\n",
    "###*but WE'RE GONNA GO *RIGHT BACK* AND *SEPARATE AGAIN* RIGHT AFTER WE SHUFFLE!!!!!!\n",
    "#but then we're gonna OVERWRITE the OLD X/y train/tests!!!\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line up the test images with their labels\n",
    "\n",
    "Xy_test_shuffle = []\n",
    "\n",
    "for i,j in enumerate(X_test):\n",
    "    new_array = (j, y_test[i])\n",
    "    Xy_test_shuffle.append(new_array)\n",
    "    \n",
    "Xy_test_shuffle = np.array(Xy_test_shuffle)\n",
    "\n",
    "#shuffle\n",
    "np.random.shuffle(Xy_test_shuffle)\n",
    "\n",
    "#BUT DO WE EVEN NEED TO SHUFFLE THE TEST SET?!?!\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmmm - ... - ahhhh okay - so i guess it's pretty hard to see in this view w/ so many values/pixels but the VERY LAST thing is what i was looking for -\n",
    "#you see that 0 in the bottom right corner? *THAT MUST BE THE Y-LABEL*!!!!! that's why it's outside alllll the other brackets all on its own/lone/lownsome!!!!!\n",
    "###########################################################################################################################################################\n",
    "# but also - WHY ARE THERE SO MANY *ZEROS*?!? well, 0 is white, so...\n",
    "#could be that this pic has alotta white pixels??\n",
    "###########################################################################################################################################################\n",
    "#Also - IS IT SUPPOSED TO BE AN *ARRAY WITHIN AN ARRAY*?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_test_shuffle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that test & train are each shuffled, separate the y labels back off from each of them since model training/testing/predicting requires\n",
    "#separation of X & y!\n",
    "#peel the meat off the bone\n",
    "\n",
    "#but i guess since this isn't a dataframe, and maybe *can't* be a dataframe? it's not easy to separate...\n",
    "#unlesss,... IF WE MADE *EACH* OF RGB ITS OWN COLUMN!!!!!!\n",
    "\n",
    "#but, otherwise - this what we'll have to do:\n",
    "\n",
    "#we already have these arrays w/ these names, but remem they contained ONLY X/y, respectively, and obviously weren't SHUFFLED at the time!\n",
    "#so we're just gonna replace/overwrite these w/ synchronized-shuffled pairs independently!\n",
    "X_train_shuffled = []\n",
    "y_train_shuffled = []\n",
    "\n",
    "#loop thru the combined/lined up training set, and send the image to X_train and the label to y_train!!!!\n",
    "for i in Xy_train_shuffle:  #remem, X_train_shuffle is a set of shuffled up PAIRS of images w/ their corresponding values\n",
    "                           #so that's the great thing - we don't risk mixing up the data since we're shuffling PAIRS!!! keeping them intact\n",
    "    \n",
    "    #so each 'i' is a tuple pair of (image, value)\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    image = i[0] #ohh okay, so this is referencing the 0-index of EACH i-element-tuple-pair within X_train_shuffle,\n",
    "    #which is the IMAGE!! cuz i([[0],[1]]) = X_train_shuffle([[0],[1]]) = X_train_shuffle([[image],[value]])!!!\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1] #and so the 1-index-position of each tuple-pair-element in X_train_shuffle is the value!!!\n",
    "    \n",
    "    # Append the values and the labels to separate arrays\n",
    "\n",
    "    #REALLY WE COULD *SKIP* THE ABOVE/HELPER AND JUST GO *DIRECTLY* TO THE APPEND, ONCE AGAIN?!\n",
    "    X_train_shuffled.append(image)\n",
    "    y_train_shuffled.append(label)\n",
    "\n",
    "#so it's just peeling apart / separating out the pair into it's parts! and putting into the respective bucket/bins/indep lists!\n",
    "#: 1st element / value / X / image into the X list and 2nd element / label / y / classifier to the y list!\n",
    "\n",
    "#hmmm okay, so now we store ALL the X_train images in one list(? or array?), SHUFFLED\n",
    "#oh - check below - again, not shure why didn't make arrays directly up here but yeah, these above as they stand are LISTS!\n",
    "#and do the same for the y-values\n",
    "#BUT REMEMBER! they were shuffled TOGETHER! as PAIRS! so these will STILL line up if we wanna join em back together!\n",
    "#and if they're lists we can even re-tuple/combine them now easily!\n",
    "\n",
    "\n",
    "#but again - question is - why do we wanna separate em into diff lists? i get it tho that if this is something we wanted\n",
    "#to do, we couldn't have simply taken these lists independently CUZ THE WHOLE POINT WAS TO TUPLE-PAIR-TIE THEM!!!\n",
    "    \n",
    "#but is it necessary to do a for loop to get the 'columns' of this tupled array? hmm doesn't seem so - remem how this\n",
    "#'tupled' array object looks like? seems to behave how you'd expect off that - nothing you can really do w/ it?\n",
    "\n",
    "\n",
    "X_train_shuffled = np.array(X_train_shuffled)\n",
    "\n",
    "y_train_shuffled = np.array(y_train_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohhhhhhh, interesting....... so we 'converted' these to just *2* dimensions?.... i was thinking since we added on y labels, that we made like a *5th*\n",
    "#dimension!?! lol, but i guess we simplified it and like 'compressed' it to a 2dimensional shape, even tho you still see the many sets of brackets\n",
    "#but yeah, i guess now it's SIMPLE! the dimensions represent the 47 test images, and the 2 is....???\n",
    "###########################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_train_shuffle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okay, at least that's what we expected, meaning based on what we just saw w/ test shuffle, aka only the first term is changing and reflects\n",
    "#the number of samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for test\n",
    "\n",
    "X_test_shuffled = []\n",
    "y_test_shuffled = []\n",
    "\n",
    "# Start a for loop into the X_test_shuffle\n",
    "for i in Xy_test_shuffle:\n",
    "    # The array containing the picture would be the one that is in the index 0\n",
    "    image = i[0]\n",
    "    # The label would be the array that is on the index 1\n",
    "    label = i[1]\n",
    "    # Append the values and the labels to separate arrays\n",
    "    X_test_shuffled.append(image)\n",
    "    y_test_shuffled.append(label)\n",
    "\n",
    "X_test_shuffled = np.array(X_test_shuffled)\n",
    "\n",
    "y_test_shuffled = np.array(y_test_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 28, 28, 3)\n",
      "(138,)\n",
      "(34, 28, 28, 3)\n",
      "(34,)\n"
     ]
    }
   ],
   "source": [
    "# Make sure shapes are the same as their originals\n",
    "#bc at this point, now that we SEPARATED again, we're BACK TO WHERE WE STARTED in terms of SHAPE/SIZE!!! w/as the originalal X/y train/test!!!\n",
    "#bc all we did after all/that was essentially SHUFFLE it!!! so these are all the SAME *SET* of data as their original, - didn't alter / edit / add /\n",
    "#remove any data/points!, - they're just in a different ORDER!!!\n",
    "#and the great thing is that, even tho they're SEPARATED, they're STILL *LINED UP*!!!!\n",
    "#lol, like when a severed hand (Allah protect us!!!!) can still move/be controlled by the brain! (is that true?)\n",
    "\n",
    "#we could've given these new names ofc btw, instead of overwrite, but i guess one approach / preference / advantage is having LESS variables!\n",
    "#other times/perspective is that's GOOD cuz advantage is keeping things SEPARATE and keeping a TRACK/RECORD / PAPER TRAIL!\n",
    "\n",
    "#if give new names above so we can differentiate and keep rather than overwrite and compare side by side here!\n",
    "#but if not, just scroll up or do side-by-side to when we did it last!\n",
    "#can make nice like put text like: print('The shape of X_train originally was %X_train.shape% and now it is %X_train_shuffled.shape%')\n",
    "print(X_train_shuffled.shape)\n",
    "print(y_train_shuffled.shape)\n",
    "print(X_test_shuffled.shape)\n",
    "print(y_test_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for neural networks\n",
    "\n",
    "def neural_network():  #if it has no args then that's really easy - will just execute code body as is! that means there's no OUTSIDE variables / params it's dependent on!\n",
    "    model = Sequential()\n",
    "    #dox: \"Sequential groups a linear stack of layers into a tf.keras.Model.\"\n",
    "    #\"A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\"\n",
    "    #so i guess for us, that's One Image, One Label??\n",
    "    ############################################\n",
    "    #interestingly, Sequential has very very little args, which is prob why we usually see it BLANK like here!\n",
    "    #and why it seems to be the norm to do what he does below, which is \"ADD\" THINGS TO IT!!!!\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu', kernel_initializer='he_uniform', input_shape=(28, 28, 3))) #don't remember why, but i tried it with removing kernel initializer to let it default, which apparently is to 'glorot_uniform'?\n",
    "    #About Conv2D:\n",
    "#\"2D convolution layer (e.g. spatial convolution over images)\"\n",
    "#\"This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.\"\n",
    "    \n",
    "    #32 should be the FILTERS - but not sure what that is... says: \"Integer, the dimensionality of the output space (i.e. the number of output filters\n",
    "        #in the convolution).\"\n",
    "        #SO NOT SURE HOW HE GET 32?!? AND WHAT IT'D BE FOR US?!\n",
    "        ############################################\n",
    "    #(3,3) should be 'kernel size', for the 'height/width' of the 2D 'convolution window'....\n",
    "        #so don't know how to determine what this should be!\n",
    "        ############################################\n",
    "    #activation - the activation function to use. don't know what this means.... &don't know what other choices there are. won't use any if don't specify\n",
    "    ############################################\n",
    "    #padding: not sure what padding is, but says: \" \"same\" results in padding with zeros evenly to the left/right or up/down of the input.\"\n",
    "    #input_shape is just the shape of the images! 28x28 RGB pixels!\n",
    "\n",
    "    #AND THERE ARE MANY OTHER ARGS JUST FOR THIS!!!!!\n",
    "\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    #a little on MaxPooling2D:\n",
    "    #\"Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window\n",
    "    #(of size defined by pool_size) for each channel of the input. The window is shifted by strides along each dimension.\"\n",
    "    #the (2,2) should be 'pool_size'\n",
    "    #\"integer or tuple of 2 integers, window size over which to take the maximum. (2, 2) will take the max value over a 2x2 pooling window.\n",
    "    #If only one integer is specified, the same window length will be used for both dimensions.\"\n",
    "        #lol so we could just write 2 ALONE?! like, not even in parenths?! ;D\n",
    "        #but yeah - no idea what this means - what's a window/size, what max??\n",
    "        #WHAT VALUE SHOULD WE DO?!\n",
    "        ########################################################################\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #\"Flattens the input. Does not affect the batch size.\"\n",
    "    #only arg this has is 'data_format', which defaults to 'channels_last'\n",
    "    #don't know what this really means\n",
    "    ########################################################################\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) #don't remember why, but i tried it with removing kernel initializer to let it default, which apparently is to 'glorot_uniform'?\n",
    "    #\"Just your regular densely-connected NN layer\"\n",
    "    #\"Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as\n",
    "    #the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer\"\n",
    "    # 128 is the 'units': \"Positive integer, dimensionality of the output space.\" ? -> but SAME THING that *FILTERS* said above!in Conv2D\n",
    "    #again, no idea what this really means or what these args do and if these vals are appropriate here, or if we needa specify MORE/different ones...\n",
    "    ########################################################################\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    #oh.... so, we meet again? so is it like for every 'dense layer' we want, we just add another line for it separately??\n",
    "    #why 1? why diff activation??\n",
    "\n",
    "    #man, this'll be hard to GridSearch! how do you do it when you're working with *ADDS* and not NATIVE ARGS TO THE MODEL THEMSELVES?!?!\n",
    "    ######################################################################################################################################################\n",
    "\n",
    "    #optimizer\n",
    "    opt = SGD(lr=0.001, momentum=.9)\n",
    "    #\"Gradient descent (with momentum) optimizer.\"\n",
    "    #has a bunch of args... how do i know what to pick?! and idk how you'd GridSearchCV this, cuz again this is ANOTHER SEPARATE THING OUTSIDE OF THE\n",
    "    #MODEL ITSELF!\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #optimizer goes here\n",
    "    #loss function - THIS IS WHAT WE WANT TO *MINIMIZE*!!!! LIKE IN REGRESSION THERE'S MSE!!!\n",
    "    #never heard of binary_crossentropy, but it's in the list of 'probabilistic losses'... stuff i recognize, like MSE, is in the 'Regression Losses' section!\n",
    "    #OHHH - so R2 and Accuracy wouldn't be in 'Loss' cuz these are NEGATIVES, aka telling us what we DIDN'T get, aka the GAP! THAT'S WHY WE WANNA\n",
    "    #*MINIMIZE* THEM!!!!! r2/accuracy we wanna MAXIMIZE!!! and so that's where *METRIC*, the next arg, COMES IN!!!!!\n",
    "    #gives us that flexibility there! metrics can go EITHER direction, positive or negative, aka what you got or what you didn't get\n",
    "    #metrics are what we know for scoring, Accuracy, MSE, etc. so then yeah, like you could have MSE in/for both loss & metrics?\n",
    "    #oh but wait - can you ever get BOTH MSE &AND* ACCURACY?! cuz accuracy is for DISCRETE/binary stuff, vs. MSE is for CONTINUOUS!\n",
    "    #OH! well, i guess you COULD still get MSE for binary like if you put it on a CONTINUUM like bayes/logistic where you have a PROBABILITY b/w 0 to 1\n",
    "    #to show how close to 0 &/vs. how close to 1?! cuz yeah like if you had 2 models you wanted to compare, don't just do raw binary accuracy, but HOW CLOSE\n",
    "    #WERE THEY TO ACTUALS! like how if there's a pass/fail exam, or two kids graduated,\n",
    "    #THEY MAY STAND/RANK VERY DIFFERENTLY ON THAT SPECTRUM OF \"PASS\"!!!\n",
    "    ######################################################################################################################################################\n",
    "    #but say just theoretically you have loss as say MSE but metrics as Accuracy, then it's still gonna only optimize on/minimize MSE, but it'll *RECORD* Accuracy too?\n",
    "    #but note, this isn't like *CROSS-VALIDATION*!!! it's just gonna tell you what it is for THIS very particular model and hyper/param combination!\n",
    "    #you're on  your own if you wanna cross-validate and compare accuracies to determine the best model!!\n",
    "\n",
    "    #and there's others/args\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sA - w/ all these variables/args, WE COULD RUN A GRIDSEARCH kfCV TO SEE WHICH DOES BEST!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x28c307ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x28c307ca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 15:29:42.442782: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-01-17 15:29:42.442949: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6768 - accuracy: 0.5816\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7116 - accuracy: 0.5646\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6144 - accuracy: 0.6461\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5748 - accuracy: 0.6485\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4877 - accuracy: 0.8342\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4265 - accuracy: 0.8330\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3811 - accuracy: 0.9017\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3528 - accuracy: 0.8996\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8998\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3046 - accuracy: 0.9419\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.3303 - accuracy: 0.8429\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2803 - accuracy: 0.9445\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2573 - accuracy: 0.9269\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.2332 - accuracy: 0.9756\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.9430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28e2231c0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "#little confusing - so we're naming our call of the neural_network function as 'model'\n",
    "#but in the function model = Sequential()\n",
    "#and then everything is built on top of / supplemented to 'model'\n",
    "#like what would happen if we named the below something else? is it like w/ the way we wrote the function they/it has\n",
    "#to match?\n",
    "\n",
    "model = neural_network() #so we're relegating/delegating RIGHT to our function! which is a *SEQUENTIAL()* NEURAL NETWORK()!!!!! w/ all those fixin's hon!\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train_shuffled, y_train_shuffled, epochs = 15)\n",
    "\n",
    "#epochs are the number of like runs?\n",
    "#default=1\n",
    "#is this what we saw w/ NUTS??\n",
    "#About .fit() in general:\n",
    "#\"Trains the model for a fixed number of epochs (iterations on a dataset).\"\n",
    "#About epochs arg:\n",
    "#\"An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch,\n",
    "#epochs is to be understood as \"final epoch\".The model is not trained for a number of iterations given by epochs,\n",
    "#but merely until the epoch of index epochs is reached.\"\n",
    "#so the last part seems to contradict the intro right above it?! about .fit()?!\n",
    "####################################################################################################################################\n",
    "#so it's how many times it goes thru ALL the data...but like, isn't it just supposed to go thru the data....once.....?\n",
    "#yeah still not clear on what epoch is...\n",
    "#seems like it's tryna say it's not 'TRAINING' on the data this many times, which yeah makes sense, but then it seems like it's saying it's the number\n",
    "#of iterations THRU the data, aka ALL of the data.....\n",
    "#so yeah, why then would you needa go thru it more than once?\n",
    "#UNLESSSSS...it's saying it's like the number of like SEGMENTS/*CHUNKS* WE WANNA BREAK UP THE DATASET INTO?!?\n",
    "#in NUTS it was like the number of INDEPENDENT SIMULATIONS! like same PARAMS, but cuz of RANDOMNESS, good to see what happens if you do again, just to make\n",
    "#sure it's consistent and what you're getting isn't some serious FLUKE! so typically you'd do 2 or - actually i guess it makes sense to do *3*, cuz it could\n",
    "#happen that ONE is a very rare fluke, but if you only do *2*, you may not know WHICH ONE!? but if you do 3, you'll know which is the ODD MAN OUT!!!!\n",
    "####################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohhh, interesting - so it tells you the loss & accuracy metrics right here!\n",
    "#AHHH - interesting - *THE GENERAL TREND IS THAT IT GETS *BETTER* W/ EACH EPOCH!*\n",
    "#aka accuracy goes up and loss goes down! improves *ALOT* by the end!!!!\n",
    "#very last one tho regresses just ever so slightly, not signif\n",
    "#WHY IS THAT/HOW DOES IT WORK?!\n",
    "#this makes it seem like it *IS* redoing the WHOLE thing each time and like training/retraining itself to \"self-improve\"!!!\n",
    "#rather than the 'chunks' idea!\n",
    "#also notice the '5/5' in each! is that like... FOLDS?!\n",
    "#remem tho - this is only TRAINING!!!/FITTING! so how do we have loss/accuracy? well we know we CAN have that w/ the training set,\n",
    "#like fit the training, so is that what it's doing?!\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm so even tho i disabled warnings, still got some here!:\n",
    "# Epoch 1/15\n",
    "# WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2940840d0> and will run it as-is.\n",
    "# Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
    "# Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
    "# To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
    "# WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x2940840d0> and will run it as-is.\n",
    "# Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
    "# Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
    "# To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
    "\n",
    "# 2023-01-17 10:15:55.484304: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
    "# 2023-01-17 10:15:55.484635: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oh lol, it does say within that:\n",
    "#\"To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x28e49d430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x28e49d430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# Predict the test set\n",
    "\n",
    "predictions = model.predict(X_test_shuffled)\n",
    "\n",
    "# So this model, like Bayes/Logistic, won't give us straight BINARY 0/1 PREDICTIONS, but rather a *PROBABILITY*!!!! CONTINUOUS FROM 0.0 TO 1.0!\n",
    "# So we need to get a BINARY version of the predictions using rounding\n",
    "# And this ofc can be custom of our choice - doesn't have to be @/on .5! we can play around w/ this to see if it helps us!?\n",
    "################################################################################################################################################\n",
    "\n",
    "#give it its own list\n",
    "binary_values = []\n",
    "\n",
    "#get a binary equivalent for each continuous predictions iteratively\n",
    "for i in predictions:\n",
    "    if i < 0.5:\n",
    "        binary_values.append(0)\n",
    "    if i >= 0.5:  ############OH WAIT! can't we just simplify this to 'else binary_values.append(1)'?!?!?!?############\n",
    "        binary_values.append(1)\n",
    "\n",
    "#####SO THIS IS ANOTHER THING WE COULD DO CROSS-VALIDATION ON!!!!*******\n",
    "\n",
    "###haha sweet sweet innocent beginnings :')\n",
    "#but how does it know what a 0 means and what a 1 means? like we assigned the 0's and 1's earlier for/as the\n",
    "#correct answers. but how does it know to call a flip a 1 and a don't flip a 0?\n",
    "#ohhh okay i get it! so we TRAINED it to identify things as either a 0 or a 1!!! cuz it can only understand numbers!\n",
    "#it has no idea what they actually are or what they mean/represent!!! thus its PREDICTIONS will be like a weighted (is that the right word? but you've got the right idea/on the right track) #lollll - as teachers said in elementary school: \"on the right track, but on the wrong train\" ;P!!!!!\n",
    "#average of how much it leans toward it being a 0/notflip or 1/flip. and we force it to pick one cuz ultimately it\n",
    "#needs to make a decision. in the final analysis of accuracy since there's diff metrics i'm sure there's one that looks\n",
    "#at the actual value to see how close it was >> LIKE *MSE*!!!. kinda like pass/fail but there of course still has to be an actual score!\n",
    "#it's just that you have to draw a line/make a cutoff somewhere\n",
    "        \n",
    "\n",
    "#so like if we're classifying 2 things/choices, it can only be one or the other, but it will spit back\n",
    "#like *HOW MUCH IT THINKS/leans toward it being one or the other*!!!!, and so we have to tell it at what point/percentage/\n",
    "#confidence we want it to make a call for one or the other, what's the defining/differentiating/determining/identifying/distinguishing/DECISION line?!!!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same warning displayed as (the first/main/big portion of) above:\n",
    "# WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x294774550> and will run it as-is.\n",
    "# Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
    "# Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
    "# To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
    "# WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x294774550> and will run it as-is.\n",
    "# Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
    "# Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
    "# To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#confusion matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_shuffled, binary_values)\n",
    "#so the y_test is now giving the ANSWER KEY so the algorithm can see how it did / its performance on this test/section\n",
    "#so we gave the algo/machine this test set of images, combined flip and not flip so it didn't know which was which,\n",
    "#all it had was the model it was trained on w/ the training set of images and corresponding labels. that was the\n",
    "#instruction/guided section. this was its turn to do it on its own/try its hand at it, no more open-book/hand-holding,\n",
    "#to see how well it learned/how well it can do on its OWN! on 'un(before)seen data'!/problems\n",
    "#so you see that it's actually very simple here, it's comparing the machine's answers, in 'binary_values', defined/\n",
    "#assigned above, and comparing them to the CORRECT ANSWERS, as defined by us in y_test!!! confusion matrix is simply\n",
    "#just a visual tool to help us see the performance w/ machine learning prediction performance\n",
    "\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['dont flip','flip'])\n",
    "cmd.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so of the 27 test dontflips, we correctly got 19. so 19/27  70%\n",
    "# & of the 20 test DO-FLIPS,  we correctly got 15, so 15/20 = 75%!\n",
    "#okay, well, aH, at least pretty CONSISTENT!\n",
    "\n",
    "#we can look at which ones IT GOT WRONG!!!! to see if there are any PATTERNS on something that's THROWING IT OFF!?!?\n",
    "#(and also which it got RIGHT!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OHHH - WE SHOULDA TRIED TO MAKE THEM MORE *EVEN* FOR EACH MAYBE?!?! like number of flips/non-flips in both training & test to avoid RATIO/CLASS IMBALANCE/BIAS!!!!\n",
    "################################################################################################################################################\n",
    "#so like we SHOULDN'T have it train on numbers that reflect the actual POPULATION!?! cuz like - then it might kinda go into that mode of like when\n",
    "#you see like 4 answer choice C's in a row, like no - can't be 'C' again! or if smarter, think like, ONE OF THESE must not be C!!!\n",
    "#it's gotta have that / any noise like that COMPLETELY OUT OF ITS MIND!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75        17\n",
      "           1       0.74      0.82      0.78        17\n",
      "\n",
      "    accuracy                           0.76        34\n",
      "   macro avg       0.77      0.76      0.76        34\n",
      "weighted avg       0.77      0.76      0.76        34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification Report to check metrics like Accuracy\n",
    "print(classification_report(y_test_shuffled, binary_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yep, so, makes perfect sense - avg b/w 70 & 75 is 72/73!\n",
    "\n",
    "#*ALSO*! in the TRAINING EPOCHS, we got a range of accuracy from .52 to .90, which *ALSO AVGS TO AROUND THE SAME*!!!! OF/@ 71.5%!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember it's not just like - if it looks like these, FLIP, if not, DON'T!\n",
    "#it's if it looks like THESE OTHER ONES, DON'T FLIP!!!!\n",
    "#aka, even though it's binary, YOU STILL GOTTA SHOW IT EXAMPLES OF WHAT *NOT* TO DO!!!!\n",
    "#cuz like there may be other areas in between that we're NOT covering!!! we're focused on a specific RANGE!\n",
    "#it's just like how WE/a KID learns!!! also needa be shown *EXAMPLES* of what *NOT* to do!!!!\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what's the best way to see which images it identified as which, esp the wrong ones?\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to create a model with 0.__ accuracy for whether a page needs to be flipped or not by using deep learning and doing the necessary data preprocessing such as making all the images black & white, all the same size, cropping them, adjusting brightness, adding noise, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x296162940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x296162940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: flip_page_classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 15:29:43.669046: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "# Save the model using pickle\n",
    "\n",
    "#doesn't say pickle anywhere??? lol we did import it at start/top tho\n",
    "\n",
    "model_classifier = model.save('flip_page_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#errors?^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c476d5be062af8c53c5a2e4f951b2f07aa9c47f05899e43528a06f0dd12534bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
